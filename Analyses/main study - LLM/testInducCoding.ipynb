{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Generic LLM helpers:\n",
    "\n",
    "- querying with a list of multiple prompts\n",
    "- caching LLM responses to save API costs\n",
    "- tokenization\n",
    "- handling different APIs\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import random\n",
    "import pandas as pd\n",
    "import os\n",
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "import httpx\n",
    "import numpy as np\n",
    "import pickle\n",
    "import hashlib\n",
    "import json\n",
    "import scipy\n",
    "import shutil\n",
    "import tiktoken\n",
    "import re\n",
    "import asyncio\n",
    "import time\n",
    "import json\n",
    "from itertools import chain\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "\n",
    "# globals: OpenAI client instances\n",
    "client = None\n",
    "embed_client = None\n",
    "client_async = None\n",
    "API_type = None\n",
    "__all__=[\"API_type\",\"client\",\"embed_client\",\"client_async\"]\n",
    "\n",
    "'''\n",
    "Set up rewriting the base path with Aalto mappings\n",
    "For all endpoints see https://www.aalto.fi/en/services/azure-openai#6-available-api-s\n",
    "'''\n",
    "# prior to making any Aalto API requests, we will update this with the desired model's OpenAI name\n",
    "current_openai_model = None\n",
    "# mapping from OpenAI model names to Aalto API URLs\n",
    "openai2aalto = {\n",
    "    \"gpt-3.5-turbo\": \"/v1/chat\",\n",
    "    \"gpt-4-turbo\": \"/v1/openai/gpt4-turbo/chat/completions\",\n",
    "    \"gpt-4o\": \"/v1/openai/gpt4o/chat/completions\",\n",
    "    \"text-embedding-3-large\": \"/v1/openai/text-embedding-3-large/embeddings\",\n",
    "    \"text-embedding-ada-002\": \"/v1/openai/ada-002/embeddings\"\n",
    "}\n",
    "\n",
    "\n",
    "def update_base_url_for_aalto(request: httpx.Request) -> None:\n",
    "    '''\n",
    "    A callback that the Aalto OpenAI clients will use to append the base API url with model URL\n",
    "    '''\n",
    "    if request.url.path == \"/chat/completions\":\n",
    "        if current_openai_model not in openai2aalto:\n",
    "            raise Exception(f\"Model {current_openai_model} not available via the Aalto API\")\n",
    "        request.url = request.url.copy_with(path=openai2aalto[current_openai_model])\n",
    "\n",
    "\n",
    "def init(API):\n",
    "    '''\n",
    "    This must be called before calling QueryLLM or other methods that make GPT API calls.\n",
    "\n",
    "    :param API: Either \"OpenAI\" or \"Aalto\"\n",
    "    '''\n",
    "    global client\n",
    "    global embed_client\n",
    "    global client_async\n",
    "    global API_type\n",
    "    API_type = API\n",
    "\n",
    "    if API == \"OpenAI\":\n",
    "        assert (\n",
    "                \"OPENAI_API_KEY\" in os.environ and os.environ.get(\"OPENAI_API_KEY\") != \"\"\n",
    "        ), \"you must set the `OPENAI_API_KEY` environment variable.\"\n",
    "        client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "        embed_client = client\n",
    "        client_async = openai.AsyncOpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "    elif API == \"Aalto\":\n",
    "        # create chat client\n",
    "        assert (\n",
    "                \"AALTO_OPENAI_API_KEY\" in os.environ and os.environ.get(\"AALTO_OPENAI_API_KEY\") != \"\"\n",
    "        ), \"you must set the `AALTO_OPENAI_API_KEY` environment variable.\"\n",
    "\n",
    "        client = openai.OpenAI(\n",
    "            base_url=\"https://aalto-openai-apigw.azure-api.net\",\n",
    "            api_key=False,  # API key not used, and rather set below\n",
    "            default_headers={\n",
    "                \"Ocp-Apim-Subscription-Key\": os.environ.get(\"AALTO_OPENAI_API_KEY\"),\n",
    "            },\n",
    "            http_client=httpx.Client(\n",
    "                event_hooks={\"request\": [update_base_url_for_aalto]}\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # create embedding client\n",
    "        auth_headers = {\n",
    "            'Ocp-Apim-Subscription-Key': os.environ.get(\"AALTO_OPENAI_API_KEY\")\n",
    "        }\n",
    "        embed_client = AzureOpenAI(\n",
    "            api_key=\"not_in_use\",  # This attribute is required but it is not in use\n",
    "            api_version=\"2024-06-01\",\n",
    "            azure_endpoint=\"https://aalto-openai-apigw.azure-api.net/v1/\",\n",
    "            default_headers=auth_headers\n",
    "        )\n",
    "        '''\n",
    "        assert (\n",
    "                \"OPENAI_API_KEY\" in os.environ and os.environ.get(\"OPENAI_API_KEY\") != \"\"\n",
    "        ), \"you must set the `OPENAI_API_KEY` environment variable.\"\n",
    "\n",
    "\n",
    "        #Aalto embedding models give an error: {'statusCode': 404, 'message': 'Resource not found'}\n",
    "        #For now, we use OpenAI embeddings instead, which should be GDPR-safe because we only embed codes instead of the raw data\n",
    "        assert (\n",
    "                \"OPENAI_API_KEY\" in os.environ and os.environ.get(\"OPENAI_API_KEY\") != \"\"\n",
    "        ), \"you must set the `OPENAI_API_KEY` environment variable.\"\n",
    "        embed_client = openai.OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "\n",
    "        #Asynchronous processing disabled for now, as it gives weird errors with Aalto models\n",
    "        client_async=None\n",
    "\n",
    "        client_async = openai.AsyncAzureOpenAI(\n",
    "            base_url=\"https://aalto-openai-apigw.azure-api.net\",\n",
    "            api_key=os.environ.get(\"AALTO_OPENAI_API_KEY\"), #False, # API key not used, and rather set below\n",
    "            api_version=\"2023-05-15\",\n",
    "            default_headers = {\n",
    "                \"Ocp-Apim-Subscription-Key\": os.environ.get(\"AALTO_OPENAI_API_KEY\"),\n",
    "            },\n",
    "            http_client=httpx.AsyncClient(\n",
    "                event_hooks={ \"request\": [update_base_url_for_aalto] }\n",
    "            ),\n",
    "        )\n",
    "        '''\n",
    "    else:\n",
    "        raise Exception(f\"Invalid LLM API: {API}\")\n",
    "\n",
    "\n",
    "# progress bar helper\n",
    "def print_progress_bar(iteration, total, prefix='', suffix='', decimals=1, length=100, fill='â–ˆ', printEnd=\"\\r\"):\n",
    "    \"\"\"\n",
    "    Call in a loop to create terminal progress bar\n",
    "    @params:\n",
    "        iteration   - Required  : current iteration (Int)\n",
    "        total       - Required  : total iterations (Int)\n",
    "        prefix      - Optional  : prefix string (Str)\n",
    "        suffix      - Optional  : suffix string (Str)\n",
    "        decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "        length      - Optional  : character length of bar (Int)\n",
    "        fill        - Optional  : bar fill character (Str)\n",
    "        printEnd    - Optional  : end character (e.g. \"\\r\", \"\\r\\n\") (Str)\n",
    "    \"\"\"\n",
    "    percent = (\"{0:.\" + str(decimals) + \"f}\").format(100 * (iteration / float(total)))\n",
    "    filledLength = int(length * iteration // total)\n",
    "    bar = fill * filledLength + '-' * (length - filledLength)\n",
    "    print(f'\\r{prefix} |{bar}| {percent}% {suffix}', end=printEnd)\n",
    "    # Print New Line on Complete\n",
    "    if iteration == total:\n",
    "        print()\n",
    "\n",
    "\n",
    "cache_dir = \"./_LLMCode_cache\" #os.path.join(os.path.dirname(os.path.realpath(__file__)), \"../_LLM_cache\")\n",
    "\n",
    "\n",
    "def cache_keys_equal(key1, key2):\n",
    "    if (type(key1) is np.ndarray) and (type(key2) is np.ndarray):\n",
    "        return np.array_equal(key1, key2)\n",
    "    return key1 == key2\n",
    "\n",
    "\n",
    "def cache_hash(key):\n",
    "    return hashlib.md5(key).hexdigest()\n",
    "\n",
    "\n",
    "def load_cached(key):\n",
    "    cached_name = cache_dir + \"/\" + cache_hash(key)\n",
    "    if os.path.exists(cached_name):\n",
    "        cached = pickle.load(open(cached_name, \"rb\"))\n",
    "        if cache_keys_equal(cached[\"key\"], key):\n",
    "            # cache_copy_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)), \"cache_copy\") #for debugging which files are actually used...\n",
    "            # shutil.copy(cached_name, cache_copy_dir+\"/\" + cache_hash(key))\n",
    "            return cached[\"value\"]\n",
    "    return None\n",
    "\n",
    "\n",
    "def cache(key, value):\n",
    "    if not os.path.exists(cache_dir):\n",
    "        os.mkdir(cache_dir)\n",
    "    cached_name = cache_dir + \"/\" + cache_hash(key)\n",
    "    pickle.dump({\"key\": key, \"value\": value}, open(cached_name, \"wb\"))\n",
    "\n",
    "\n",
    "tiktoken_encodings = {\n",
    "    \"gpt-4o\": tiktoken.get_encoding(\"cl100k_base\"),\n",
    "    \"gpt-4o-mini\": tiktoken.get_encoding(\"cl100k_base\"),\n",
    "    \"gpt-4-turbo\": tiktoken.get_encoding(\"cl100k_base\"),\n",
    "    \"gpt-4-turbo-preview\": tiktoken.get_encoding(\"cl100k_base\"),\n",
    "    \"gpt-4\": tiktoken.get_encoding(\"cl100k_base\"),\n",
    "    \"gpt-3.5-turbo\": tiktoken.get_encoding(\"cl100k_base\"),\n",
    "    \"gpt-3.5-turbo-instruct\": tiktoken.get_encoding(\"cl100k_base\"),\n",
    "    \"gpt-3.5-turbo-16k\": tiktoken.get_encoding(\"cl100k_base\"),\n",
    "    \"text-davinci-003\": tiktoken.get_encoding(\"p50k_base\"),\n",
    "    \"text-davinci-002\": tiktoken.get_encoding(\"p50k_base\"),\n",
    "    \"text-davinci-001\": tiktoken.get_encoding(\"r50k_base\"),\n",
    "    \"text-curie-001\": tiktoken.get_encoding(\"r50k_base\"),\n",
    "    \"text-babbage-001\": tiktoken.get_encoding(\"r50k_base\"),\n",
    "    \"text-ada-001\": tiktoken.get_encoding(\"r50k_base\"),\n",
    "    \"davinci\": tiktoken.get_encoding(\"r50k_base\"),\n",
    "    \"curie\": tiktoken.get_encoding(\"r50k_base\"),\n",
    "    \"babbage\": tiktoken.get_encoding(\"r50k_base\"),\n",
    "    \"ada\": tiktoken.get_encoding(\"r50k_base\"),\n",
    "}\n",
    "\n",
    "max_llm_context_length = {\n",
    "    \"gpt-4o\": 128000,\n",
    "    \"gpt-4-turbo\": 16384 * 2,\n",
    "    \"gpt-4-turbo-preview\": 16384 * 2,\n",
    "    \"gpt-3.5-turbo-16k\": 16384,\n",
    "    \"gpt-4\": 8192,\n",
    "    \"gpt-3.5-turbo\": 4096,\n",
    "    \"gpt-3.5-turbo-instruct\": 4096,\n",
    "    \"text-davinci-003\": 4096,\n",
    "    \"text-davinci-002\": 4096,\n",
    "    \"text-davinci-001\": 2049,\n",
    "    \"text-curie-001\": 2049,\n",
    "    \"text-babbage-001\": 2049,\n",
    "    \"text-ada-001\": 2049,\n",
    "    \"davinci\": 2049,\n",
    "    \"curie\": 2049,\n",
    "    \"babbage\": 2049,\n",
    "    \"ada\": 2049\n",
    "}\n",
    "\n",
    "\n",
    "def is_chat_model(model):\n",
    "    return (\"gpt-4\" in model) or (\"gpt-3.5-turbo\" in model) and (\"gpt-3.5-turbo-instruct\" not in model)\n",
    "\n",
    "\n",
    "def token_overhead(model):\n",
    "    if is_chat_model(model):\n",
    "        return 300  # these models have some overhead because of the system message and chat structure\n",
    "    return 0\n",
    "\n",
    "\n",
    "def num_tokens_from_string(string: str, model: str) -> int:\n",
    "    \"\"\"Returns the number of tokens in a text string.\"\"\"\n",
    "    if not model in tiktoken_encodings:\n",
    "        raise Exception(f\"Tiktoken encoding unknown for LLM: {model}\")\n",
    "    encoding = tiktoken_encodings[model]\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Queries an LLM for continuations of a batch of prompts given as a list\n",
    "def query_LLM_batch(model, prompt_batch, max_tokens, use_cache=None, temperature=None,system_message=None,stop=None):\n",
    "    # Print all arguments to check if any are None\n",
    "    print(\"Arguments:\")\n",
    "    print(f\"model: {model}\")\n",
    "    print(f\"prompt_batch: {prompt_batch}\")\n",
    "    print(f\"max_tokens: {max_tokens}\")\n",
    "    print(f\"use_cache: {use_cache}\")\n",
    "    print(f\"temperature: {temperature}\")\n",
    "    print(f\"system_message: {system_message}\")\n",
    "    print(f\"stop: {stop}\")\n",
    "    \n",
    "    # Handle None for system_message\n",
    "    if system_message is None:\n",
    "        system_message = \"You are a helpful assistant.\"\n",
    "    \n",
    "    # Handle None for stop - it should only be passed to the API if it's not None\n",
    "    stop = stop if stop is not None else []\n",
    "    \n",
    "    \n",
    "    global current_openai_model\n",
    "    current_openai_model = model    #needed for the Aalto Azure GPT API callbacks\n",
    "\n",
    "    if temperature is None:\n",
    "        temperature=0 #by default, operate fully deterministically\n",
    "        \n",
    "    print(\"OK 1\")\n",
    "    \n",
    "    \n",
    "    # Check for None values in API_type, model, and prompt_batch elements\n",
    "    print(\"Checking for None values in API_type, model, and prompt_batch...\")\n",
    "\n",
    "    if API_type is None:\n",
    "        print(\"API_type is None\")\n",
    "    else:\n",
    "        print(f\"API_type: {API_type}\")\n",
    "\n",
    "    if model is None:\n",
    "        print(\"model is None\")\n",
    "    else:\n",
    "        print(f\"model: {model}\")\n",
    "\n",
    "    # Check each element in prompt_batch for None\n",
    "    for i, prompt in enumerate(prompt_batch):\n",
    "        if prompt is None:\n",
    "            print(f\"prompt_batch element at index {i} is None\")\n",
    "        else:\n",
    "            print(f\"prompt_batch element at index {i}: {prompt}\")\n",
    "        \n",
    "        \n",
    "    if use_cache is None:\n",
    "        use_cache=False\n",
    "    cache_key=(API_type+\"_\"+model+\"\".join(prompt_batch)).encode('utf-8')\n",
    "    if use_cache:\n",
    "        cached_result=load_cached(cache_key)\n",
    "        if cached_result is not None:\n",
    "            return cached_result\n",
    "\n",
    "    print(\"OK 2\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    #choose whether to use the chat API or the older query API\n",
    "    if is_chat_model(model):\n",
    "        if system_message is None:\n",
    "            system_message = \"You are a helpful assistant.\"\n",
    "        if API_type==\"Aalto\" and (client_async is None):\n",
    "            # In case no async API client, fall back to running the prompts one-by-one\n",
    "            continuations=[]\n",
    "            for prompt in prompt_batch:\n",
    "                success=False\n",
    "                while not success:\n",
    "                    try:\n",
    "                        response = client.chat.completions.create(\n",
    "                            model=model,\n",
    "                            # the model variable must be set, but has no effect, model selection done in update_base_url_for_aalto\n",
    "                            messages=[\n",
    "                                {\"role\": \"system\", \"content\": system_message},\n",
    "                                {\"role\": \"user\", \"content\": prompt},\n",
    "                            ],\n",
    "                        )\n",
    "                        success=True\n",
    "                    except openai.RateLimitError:\n",
    "                        print(\"Rate limit error! Will retry in 5 seconds\")\n",
    "                        time.sleep(5)\n",
    "                    if response is None or response.choices is None:\n",
    "                        print(\"No response from API, will retry in 5 seconds. Check VPN settings if you're not in Aalto intranet.\")\n",
    "                        time.sleep(5)\n",
    "                        success=False\n",
    "                if response.choices[0].message.content is None:\n",
    "                    continuations.append(\"\")\n",
    "                else:\n",
    "                    continuations.append(response.choices[0].message.content.strip())\n",
    "        else:\n",
    "                # each batch in the prompt becomes its own asynchronous chat completion request\n",
    "            async def batch_request(prompt_batch):\n",
    "                tasks=[]\n",
    "                for prompt in prompt_batch:\n",
    "                    messages = [\n",
    "                        {\"role\": \"system\", \"content\": system_message},\n",
    "                        {\"role\": \"user\", \"content\": prompt},\n",
    "                    ]\n",
    "                    tasks.append(client_async.chat.completions.create(\n",
    "                        model=model,\n",
    "                        messages=messages,\n",
    "                        temperature=temperature,\n",
    "                        max_tokens=max_tokens,\n",
    "                        n=1,  # one completion per prompt\n",
    "                        stop=stop,\n",
    "                        frequency_penalty=0.0,\n",
    "                        presence_penalty=0.0,\n",
    "                    ))\n",
    "                return await asyncio.gather(*tasks)\n",
    "\n",
    "            loop = asyncio.get_event_loop()\n",
    "            responses = loop.run_until_complete(batch_request(prompt_batch))\n",
    "            continuations = [response.choices[0].message.content.strip() for response in responses]\n",
    "\n",
    "        print(\"OK 3\")\n",
    "        # before we return the continuations, ensure that we don't violate OpenAI's rate limits\n",
    "        total_tokens = 0\n",
    "        for prompt in prompt_batch:\n",
    "            total_tokens += num_tokens_from_string(string=system_message, model=model)\n",
    "            total_tokens += num_tokens_from_string(string=prompt, model=model)\n",
    "        for continuation in continuations:\n",
    "            total_tokens += num_tokens_from_string(string=continuation, model=model)\n",
    "        max_tokens_per_minute = 600000  # currently imposed limit for GPT-4o on tier 5 is 30 million TPM\n",
    "        wait_seconds = (total_tokens / max_tokens_per_minute) * 60.0\n",
    "        #print(f\"Waiting {wait_seconds} seconds to ensure staying within rate limit\")\n",
    "        time_elapsed=time.time() - start_time\n",
    "        if time_elapsed<wait_seconds:\n",
    "            time.sleep(wait_seconds-time_elapsed)\n",
    "\n",
    "    else:\n",
    "        # The old completions API supports batched prompts out-of-the-box\n",
    "        response = client.completions.create(\n",
    "            model=model,\n",
    "            prompt=prompt_batch,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=1.0,\n",
    "            frequency_penalty=0.0,\n",
    "            presence_penalty=0.0,\n",
    "            stop=stop,\n",
    "            n=1  # one completion per prompt\n",
    "        )\n",
    "        # extract continuations\n",
    "        continuations = [choice.text for choice in response.choices]\n",
    "\n",
    "        # before we return the continuations, ensure that we don't violate OpenAI's rate limits\n",
    "        total_tokens = 0\n",
    "        for prompt in prompt_batch:\n",
    "            total_tokens += num_tokens_from_string(string=prompt, model=model)\n",
    "        for continuation in continuations:\n",
    "            total_tokens += num_tokens_from_string(string=continuation, model=model)\n",
    "        max_tokens_per_minute = 90000  # currently imposed limit for ChatGPT models\n",
    "        wait_seconds = (total_tokens / max_tokens_per_minute) * 60.0\n",
    "        time_elapsed=time.time() - start_time\n",
    "        if time_elapsed<wait_seconds:\n",
    "            time.sleep(wait_seconds-time_elapsed)\n",
    "\n",
    "    if use_cache:\n",
    "        cache(key=cache_key,value=continuations)\n",
    "    return continuations\n",
    "\n",
    "def query_LLM(prompts, model=None, max_tokens=None, use_cache=None, temperature=None, system_message=None,stop=None):\n",
    "    \"\"\"\n",
    "        Query a Language Model (LLM) with one or more prompts.\n",
    "\n",
    "        This function sends prompts to a specified language model and returns the generated continuations for each prompt in batches.\n",
    "\n",
    "        Args:\n",
    "            prompts (str or list of str): The prompt or list of prompts to be sent to the LLM. If a single string is provided, it will be wrapped in a list.\n",
    "            model (str, optional): The identifier for the model to be queried. Defaults to \"gpt-4o\".\n",
    "            max_tokens (int, optional): The maximum number of tokens to generate in each completion. If None, the model's default will be used.\n",
    "            use_cache (bool, optional): Whether to use cached results if available. Defaults to False.\n",
    "            temperature (float, optional): Sampling temperature to use, in range (0, 1]. Higher values mean the model will take more risks. Defaults to 0.\n",
    "            system_message (str, optional): A system message that can influence the generated response. Defaults to None.\n",
    "            stop (str or list of str, optional): Sequences where the model will stop producing further tokens. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            list of str: The continuations generated by the model for each input prompt.\n",
    "\n",
    "        Examples:\n",
    "            >>> responses = query_LLM(\"What is the capital of France?\")\n",
    "            >>> print(responses)\n",
    "            [\"The capital of France is Paris.\"]\n",
    "\n",
    "            >>> prompts = [\"Translate the following sentence to French: 'Hello, world!'\", \"Translate the following sentence to Spanish: 'Good morning!'\"]\n",
    "            >>> responses = query_LLM(prompts, model=\"gpt-3.5-turbo\", max_tokens=50, temperature=0.5)\n",
    "            >>> print(responses)\n",
    "            [\"Bonjour, le monde!\", \"Â¡Buenos dÃ­as!\"]\n",
    "        \"\"\"\n",
    "    if model is None:\n",
    "        model=\"gpt-4o\"\n",
    "    if use_cache is None:\n",
    "        use_cache=False\n",
    "    if temperature is None:\n",
    "        temperature=0\n",
    "    return_single=False\n",
    "    if isinstance(prompts, str):\n",
    "        prompts=[prompts] #the following code always expects a list of prompts\n",
    "        return_single=True\n",
    "\n",
    "    #Query the LLM in batches\n",
    "    continuations=[]\n",
    "    batch_size = 10  # The exact max batch_size for each model is unknown. This seems to work for all, and provides a nice speed-up.\n",
    "    N = len(prompts)\n",
    "    print(\"N:\",N)\n",
    "    for i in range(0, N, batch_size):\n",
    "        prompt_batch=prompts[i:min([N, i + batch_size])]\n",
    "        print(\"prompt_batch for loop:\", prompt_batch)\n",
    "\n",
    "        continuations+=query_LLM_batch(model=model,\n",
    "                                 prompt_batch=prompt_batch,\n",
    "                                 max_tokens=max_tokens,\n",
    "                                 use_cache=use_cache,\n",
    "                                 temperature=temperature,\n",
    "                                 system_message=system_message,\n",
    "                                 stop=stop)\n",
    "        print(\"continuations for loop:\", continuations)\n",
    "\n",
    "        if N>batch_size:\n",
    "            print_progress_bar(min([N, i + batch_size]), N,printEnd=\"\")\n",
    "    return continuations[0] if return_single else continuations\n",
    "\n",
    "\n",
    "## TODO: Integrate with other query_LLM function\n",
    "def query_LLM_with_response_format(prompt, response_format, model=\"gpt-4o\", max_tokens=None):\n",
    "    \"\"\"\n",
    "    Queries an LLM with a specified prompt and returns the response parsed into the specified response format class.\n",
    "    Useful for when you want a structured output.\n",
    "\n",
    "    Args:\n",
    "        prompt (str): The input text prompt for the model.\n",
    "        response_format (Type[BaseModel]): The Pydantic model class to parse the response into.\n",
    "        model (str, optional): The model to use for querying (default is 'gpt-4o').\n",
    "        max_tokens (int, optional): Maximum number of tokens in the response (default is None).\n",
    "\n",
    "    Returns:\n",
    "        BaseModel: The parsed response as an instance of the specified Pydantic model class.\n",
    "    \"\"\"\n",
    "    success = False\n",
    "    while not success:\n",
    "        try:\n",
    "            response = client.beta.chat.completions.parse(\n",
    "                model=model,\n",
    "                messages=[\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                response_format=response_format,\n",
    "                max_tokens=max_tokens,\n",
    "            )\n",
    "            success = True\n",
    "        except openai.RateLimitError:\n",
    "            print(\"Rate limit error! Will retry in 5 seconds\")\n",
    "            time.sleep(5)\n",
    "    return response.choices[0].message.parsed\n",
    "\n",
    "\n",
    "def set_cache_directory(dir):\n",
    "    global cache_dir\n",
    "    cache_dir=dir\n",
    "\n",
    "def get_cache_directory():\n",
    "    return cache_dir\n",
    "\n",
    "\n",
    "\n",
    "def embed(texts,use_cache=None,model=None,verbose=True):\n",
    "\n",
    "    if model is None:\n",
    "        model = \"text-embedding-ada-002\"\n",
    "\n",
    "    if use_cache is None:\n",
    "        use_cache = True\n",
    "\n",
    "    cache_key=(API_type+\"_\"+model+\"\".join(texts)).encode('utf-8')\n",
    "    if use_cache:\n",
    "        cached_result=load_cached(cache_key)\n",
    "        if cached_result is not None:\n",
    "            if verbose:\n",
    "                print(\"Loaded embeddings from cache, hash\", cache_hash(cache_key))\n",
    "            return cached_result\n",
    "\n",
    "\n",
    "    #query embeddings from the API\n",
    "    texts=[json.dumps(s) for s in texts]  #make sure we escape quotes in a way compatible with GPT-3 API's internal use of json\n",
    "    batch_size = 32\n",
    "    N = len(texts)\n",
    "\n",
    "    embed_matrix=[]\n",
    "    for i in range(0, N, batch_size):\n",
    "        print_progress_bar(i, N, printEnd=\"\")\n",
    "        embed_batch=texts[i:min([N, i + batch_size])]\n",
    "        embeddings = embed_client.embeddings.create(input=embed_batch, model=model)\n",
    "        for j in range(len(embed_batch)):\n",
    "            embed_matrix.append(embeddings.data[j].embedding)\n",
    "    print(\"\")\n",
    "    embed_matrix=np.array(embed_matrix)\n",
    "    #dim = len(embeddings['data'][0]['embedding'])\n",
    "    #embed_matrix = np.zeros([N, dim])\n",
    "    #for i in range(N):\n",
    "    #    embed_matrix[i, :] = embeddings['data'][i]['embedding']\n",
    "\n",
    "    #update cache\n",
    "    if use_cache:\n",
    "        cache(cache_key,embed_matrix)\n",
    "\n",
    "    #return results\n",
    "    return embed_matrix\n",
    "\n",
    "\n",
    "def reduce_embedding_dimensionality(embeddings,num_dimensions,method=\"UMAP\",use_cache=True,n_neighbors=None,verbose=True):\n",
    "    if isinstance(embeddings,list):\n",
    "        #embeddings is a list of embedding matrices => pack all to one big matrix for joint dimensionality reduction\n",
    "        all_emb = np.concatenate(embeddings, axis=0)\n",
    "    else:\n",
    "        all_emb = embeddings\n",
    "    def unpack(x,embeddings_list):\n",
    "        row = 0\n",
    "        result = []\n",
    "        for e in embeddings_list:\n",
    "            N = e.shape[0]\n",
    "            result.append(x[row:row + N])\n",
    "            row += N\n",
    "        return result\n",
    "\n",
    "    cache_key=(str(all_emb.tostring())+str(num_dimensions)+method+str(n_neighbors)).encode('utf-8')\n",
    "    if use_cache:\n",
    "        cached_result=load_cached(cache_key)\n",
    "        if cached_result is not None:\n",
    "            if verbose:\n",
    "                print(\"Loaded dimensionality reduction results from cache, hash \", cache_hash(cache_key))\n",
    "            if isinstance(embeddings, list):\n",
    "                return unpack(cached_result,embeddings)\n",
    "            else:\n",
    "                return cached_result\n",
    "    from sklearn.manifold import MDS\n",
    "    from sklearn.manifold import TSNE\n",
    "    import umap\n",
    "    from sklearn.decomposition import PCA\n",
    "    #cosine distance\n",
    "    all_emb=all_emb/np.linalg.norm(all_emb,axis=1,keepdims=True)\n",
    "\n",
    "    if method==\"MDS\":\n",
    "        mds=MDS(n_components=num_dimensions,dissimilarity=\"precomputed\")\n",
    "        cosine_sim = np.inner(all_emb, all_emb)\n",
    "        cosine_dist = 1 - cosine_sim\n",
    "        x=mds.fit_transform(cosine_dist)\n",
    "    elif method==\"TSNE\":\n",
    "        tsne=TSNE(n_components=num_dimensions)\n",
    "        x=tsne.fit_transform(all_emb)\n",
    "    elif method==\"PCA\":\n",
    "        pca=PCA(n_components=num_dimensions)\n",
    "        x=pca.fit_transform(all_emb)\n",
    "    elif method==\"UMAP\":\n",
    "        if n_neighbors is None:\n",
    "            n_neighbors=5\n",
    "        reducer = umap.UMAP(n_components=num_dimensions,metric='cosine',n_neighbors=n_neighbors)\n",
    "        x=reducer.fit_transform(all_emb)\n",
    "    else:\n",
    "        raise Exception(\"Invalid dimensionality reduction method!\")\n",
    "\n",
    "    if use_cache:\n",
    "        cache(cache_key,x)\n",
    "\n",
    "    if isinstance(embeddings, list):\n",
    "        return unpack(x,embeddings)\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from Levenshtein import distance\n",
    "from collections import defaultdict\n",
    "from rapidfuzz import fuzz\n",
    "\n",
    "#Construct a coding prompt for a single text (unit of analysis)\n",
    "def construct_prompt(coding_instruction,few_shot_texts,few_shot_codes,codes_so_far,text):\n",
    "    # The prompt always starts with the user-defined coding instruction\n",
    "    prompt = coding_instruction + \"\\n\\n\"\n",
    "\n",
    "    # Next, we optionally add all the codes created so far, to allow code reuse instead of creating redundant\n",
    "    # new and only slightly different codes for new coded texts\n",
    "    if (codes_so_far is not None) and len(codes_so_far)>0:\n",
    "        if len(codes_so_far)>0:\n",
    "            prompt += \"Examples of codes to use. Please add new codes when needed:\\n\"\n",
    "            # Shuffle codes to mitigate LLM recency bias\n",
    "            codes_so_far=codes_so_far.copy()\n",
    "            random.shuffle(codes_so_far)\n",
    "            # Add each code as a new line\n",
    "            for code in codes_so_far:\n",
    "                prompt+=code+\"\\n\"\n",
    "\n",
    "    # Next, we add the few-shot examples separated by ###\n",
    "    # We first shuffle the examples to mitigate LLM recency bias\n",
    "    l = list(zip(few_shot_texts, few_shot_codes))\n",
    "    random.shuffle(l)\n",
    "    few_shot_texts, few_shot_codes = zip(*l)\n",
    "    for i in range(len(few_shot_texts)):\n",
    "        prompt+=\"\\n\\n###\\n\\nText: \"+few_shot_texts[i]+\"\\n\\n\"+\"Codes: \"+few_shot_codes[i]\n",
    "\n",
    "    # Finally, we add the text to code, suggesting the LLM that it should continue the prompt with the codes\n",
    "    prompt+=\"\\n\\n###\\n\\nText: \"+text+\"\\n\\n\"+\"Codes:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "# Extract a string of codes (separated by semicolons) from LLM continuation\n",
    "def continuation_to_code_string(continuation):\n",
    "    stopseq=\"###\"\n",
    "    continuation = continuation.split(stopseq)[\n",
    "        0]  # if the model started generating new texts, only keep the continuation\n",
    "    continuation = continuation.lstrip(\" \\n\")  # strip leading spaces and newlines\n",
    "    continuation = continuation.rstrip(\" ;\\n\")  # strip trailing spaces, semicolons and newlines\n",
    "    return continuation\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert from a string representation of multiple codes to a list\n",
    "def codes_to_string(codes):\n",
    "    return \"; \".join(codes)\n",
    "\n",
    "# Convert list of codes to a single string\n",
    "def string_to_codes(s):\n",
    "    codes = s.split(\";\")\n",
    "    codes = [t.strip() for t in codes]\n",
    "    return codes\n",
    "\n",
    "#Code a list of texts using an instruction string and few-shot examples of texts and codes\n",
    "def code_texts(coding_instruction,\n",
    "               few_shot_texts,\n",
    "               few_shot_codes,\n",
    "               gpt_model,\n",
    "               texts,\n",
    "               use_cache=True,\n",
    "               verbose=False,\n",
    "               max_tokens=None):\n",
    "    if max_tokens is None:\n",
    "        max_tokens=64 #we assume the codes are short. TODO: measure the token length of few-shot examples, adjust this accordingly (e.g., 2x)\n",
    "\n",
    "    #If we don't care about code consistency, we can speed things up by coding batches of texts in parallel.\n",
    "    #First, we construct the prompts for all coded texts\n",
    "    prompts=[]\n",
    "    for text in texts:\n",
    "        prompts.append(construct_prompt(coding_instruction=coding_instruction,\n",
    "                            few_shot_texts=few_shot_texts,\n",
    "                            few_shot_codes=few_shot_codes,\n",
    "                            codes_so_far=None,\n",
    "                            text=text))\n",
    "    if verbose:\n",
    "        print(f\"Constructed {len(prompts)} prompts, example: {prompts[-1]}\")\n",
    "\n",
    "    #Query the LLM\n",
    "    continuations=query_LLM(model=gpt_model,\n",
    "                            prompts=prompts,\n",
    "                            max_tokens=max_tokens,\n",
    "                            use_cache=use_cache)\n",
    "\n",
    "    # Extract code strings from the LLM continuations\n",
    "    result_codes=[continuation_to_code_string(c) for c in continuations]\n",
    "\n",
    "    # Ensure that the few-shot examples got coded correctly\n",
    "    for i,text in enumerate(texts):\n",
    "        if text in few_shot_texts:\n",
    "            few_shot_code=few_shot_codes[few_shot_texts.index(text)]\n",
    "            print(f\"Replacing code {result_codes[i]} with {few_shot_code}\")\n",
    "            result_codes[i]=few_shot_code.strip()\n",
    "\n",
    "    return result_codes\n",
    "\n",
    "\n",
    "\n",
    "def extract_single_codes(df,multiple_column=\"codes\",single_column=\"code\",count_column=\"count\",id_column=\"code_id\",merge_singular_and_plural=False):\n",
    "    df=df.copy()  #will be modified to contain the contents\n",
    "    # Parse individual codes\n",
    "    N = df.shape[0]\n",
    "    df_append = None  # replicated rows are collected in this df\n",
    "    df[single_column]=df[multiple_column].copy()\n",
    "    for i in range(N):\n",
    "        codes = df.at[i, multiple_column]\n",
    "        codes = codes.split(\";\")\n",
    "        codes = [t.lstrip() for t in codes]\n",
    "\n",
    "        # if there's more than a single topic, create copies of the entry\n",
    "        if len(codes) > 1:\n",
    "            row = df.loc[i:i].copy()  # make a temp df of the current row\n",
    "            df.at[i, multiple_column] = \"PRUNED\"  # mark the current row for pruning\n",
    "            for code in codes:\n",
    "                row[single_column] = code\n",
    "                # display(row)\n",
    "                if df_append is None:\n",
    "                    df_append = row.copy()\n",
    "                else:\n",
    "                    df_append = pd.concat([df_append,row.copy()],axis=0)\n",
    "    df = df[df[multiple_column] != \"PRUNED\"]\n",
    "    df = pd.concat([df, df_append], axis=0, ignore_index=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    codes = df[single_column].unique().tolist()\n",
    "    #print(f\"Added {df.shape[0] - N} duplicate text rows to handle texts with multiple codes\")\n",
    "    #df=df.drop(columns=[multiple_column])\n",
    "\n",
    "    # Merge singular and plural forms of the same code\n",
    "    if merge_singular_and_plural:\n",
    "        for code in codes:\n",
    "            if code + \"s\" in codes:\n",
    "                df.loc[df[single_column] == code, single_column] = code + \"s\"\n",
    "                codes.remove(code)\n",
    "\n",
    "    # Count and sort to have the most frequent topics at the top\n",
    "    df[count_column] = 0\n",
    "    counts = []\n",
    "    for code in codes:\n",
    "        count = df[df[single_column] == code].shape[0]\n",
    "        counts.append(count)\n",
    "        df.loc[df[single_column] == code, count_column] = count\n",
    "\n",
    "    df.sort_values(by=[count_column, single_column], inplace=True, ascending=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def group_codes_using_embeddings(df,embedding_context=\"\",embedding_model=\"text-similarity-curie-001\",min_group_size=3,max_group_emb_codes=100,group_desc_codes=2,group_desc_freq=True,grouping_dim=5,use_cache=True,dimred_method=None):\n",
    "    \"\"\"\n",
    "    Combines codes to groups/themes using HDBSCAN clustering of GPT-3 embedding vectors for each code.\n",
    "\n",
    "    Also counts how many coded texts were assigned with each code and group. Note that one text can be assigned to\n",
    "    multiple codes and therefore multiple groups.\n",
    "\n",
    "    Args:\n",
    "        df                  A dataframe with a \"text_id\" and \"codes\" column with codes separated by semicolons (as\n",
    "                            produced by the code() function), or a \"code\" column with single codes (as produced by\n",
    "                            the extract_single_codes() function).\n",
    "        embedding_context   A context string appended to each code when computing code embeddings. This may help\n",
    "                            disambiguating code labels that might be synonyms in the general case, but have different\n",
    "                            meaning in the coding context. E.g., \", in context of experiencing games as art\"\n",
    "        group_desc_codes    How many of the most frequent codes to use for describing each code group\n",
    "        group_desc_freq     Whether to use the group frequency in the group description\n",
    "        min_group_size      Minimum number of codes in each code group\n",
    "        use_cache           If True, skips OpenAI API calls if results found in the cache\n",
    "                            (see set_cache_directory)\n",
    "\n",
    "    Returns:\n",
    "        A dictionary of results with the following keys:\n",
    "        \"coded\"             Same as the input DataFrame but with the following additional columns:\n",
    "                            \"text_id\", \"text\", \"codes\", \"code\", \"code_id\", \"code_count\", \"group_codes\", \"group_id\",\n",
    "                            \"group_count\", \"group_desc\" columns. \"codes\" is a comma-separated list of all codes\n",
    "                            assigned to each text. \"code\" is a single code, and each text is duplicated to as many\n",
    "                            DataFrame rows as there are codes assigned to that text. code and group id:s are integers\n",
    "                            that can be used to index the embedding arrays (see below). \"text_id\" is an index to the\n",
    "                            original list of texts.\n",
    "        \"embeddings\"        A numpy array of shape [num_codes,num_embedding_dimensions]\n",
    "        \"embeddings_5d\"     Dimensionality-reduced embeddings used for clustering\n",
    "        \"group_embeddings\"  A numpy array of shape [num_groups,num_embedding_dimensions], with the (normalized)\n",
    "                            count-weighted average of each group's embedding vector. Can be useful for finding the\n",
    "                            closest code groups between two separately coded datasets\n",
    "        \"group_code_ids\"    list of lists of code ids (indices) for each group. Useful for, e.g., indexing the\n",
    "                            embeddings of each group\n",
    "\n",
    "        Note that if you want to view or list subsets of the results, e.g., the codes for each original text,\n",
    "        this is easy to do by using Pandas drop_duplicates() on a suitable column such as \"text_id\"\n",
    "    \"\"\"\n",
    "    if dimred_method is None:\n",
    "        dimred_method=\"UMAP\"\n",
    "\n",
    "    df=df.copy() #will be modified to contain the results\n",
    "\n",
    "    # extract individual codes\n",
    "    if \"text_id\" not in df.columns:\n",
    "        raise Exception(\"The input DataFrame does not have a \\\"text_id\\\" column!\")\n",
    "    if \"code\" in df.columns:\n",
    "        print(\"The input dataframe already has a \\\"code\\\" column => skipping the initial extraction of single codes.\")\n",
    "    else:\n",
    "        if \"codes\" not in df.columns:\n",
    "            raise Exception(\"The input DataFrame does not have a \\\"codes\\\" column!\")\n",
    "        df[\"codes\"] = df[\"codes\"].astype(str)\n",
    "        df = extract_single_codes(df)\n",
    "\n",
    "    codes = df[\"code\"].unique().tolist()\n",
    "\n",
    "    code_counts=df.drop_duplicates(subset=[\"code\"])[\"count\"].to_numpy()\n",
    "    n_codes=len(codes)\n",
    "    n_texts=df[\"text_id\"].nunique()\n",
    "\n",
    "    #compute code embeddings\n",
    "    print(\"Calculating code embeddings...\")\n",
    "    codes_with_context=[code + embedding_context for code in codes]\n",
    "    embeddings=embed(codes_with_context,model=embedding_model,use_cache=use_cache)\n",
    "    embed_dim=embeddings.shape[1]\n",
    "\n",
    "\n",
    "    print(\"Reducing embedding dimensionality to 2D for visualization\")\n",
    "    dimred_neighbors=min_group_size+2 #heuristic: for the grouping to work, dimensionality reduction needs to consider a somewhat larger neighborhood\n",
    "    embeddings_2d = reduce_embedding_dimensionality(embeddings,2,use_cache=use_cache,method=dimred_method,n_neighbors=dimred_neighbors)\n",
    "\n",
    "    #reduce dimensionality\n",
    "    print(\"Reducing embedding dimensionality for grouping...\")\n",
    "    embeddings_reduced=reduce_embedding_dimensionality(embeddings, grouping_dim, method=dimred_method,use_cache=use_cache,n_neighbors=dimred_neighbors)\n",
    "\n",
    "    #group (UMAP + HDBSCAN)\n",
    "    print(\"Grouping...\")\n",
    "    clustering_method='hdbscan'\n",
    "\n",
    "    if clustering_method==\"hdbscan\":\n",
    "         import hdbscan\n",
    "         cluster = hdbscan.HDBSCAN(min_cluster_size=min_group_size,\n",
    "                                   metric='euclidean',\n",
    "                                   cluster_selection_method='eom').fit(embeddings_reduced)\n",
    "         if np.min(cluster.labels_)<0:\n",
    "             has_outliers=True\n",
    "             labels=cluster.labels_+1  #+1 because the outlier cluster has id -1, which we now map to 0 to make indexing easier\n",
    "         else:\n",
    "             has_outliers = False\n",
    "             labels=cluster.labels_\n",
    "\n",
    "    elif clustering_method==\"k-means\":\n",
    "         from sklearn.cluster import KMeans\n",
    "\n",
    "         n_groups = 20\n",
    "         kmeans = KMeans(init='k-means++', n_clusters=n_groups, n_init=20)\n",
    "         kmeans.fit(embeddings_reduced)\n",
    "\n",
    "         # Predict the cluster for all the samples\n",
    "         labels = kmeans.predict(embeddings_reduced)\n",
    "\n",
    "    #Update the cluster_id column\n",
    "    n_groups = np.max(labels) + 1\n",
    "    print(f\"Combined {len(codes)} codes to {n_groups} groups\")\n",
    "    groups= [''] * n_groups\n",
    "    group_code_ids=[None] * n_groups\n",
    "    for i in range(n_groups):\n",
    "        group_code_ids[i]=[]\n",
    "\n",
    "    df[\"group_id\"]=0\n",
    "    df[\"code_id\"]=-1\n",
    "    df[\"code_2d_0\"]=0\n",
    "    df[\"code_2d_1\"]=0\n",
    "    for code_id,code in enumerate(codes):\n",
    "         #group label for this code\n",
    "         group_id=labels[code_id]\n",
    "         group_code_ids[group_id].append(code_id)\n",
    "         if groups[group_id]=='':\n",
    "              groups[group_id]=codes[code_id]\n",
    "         else:\n",
    "              groups[group_id]+=', '+codes[code_id]\n",
    "         df.loc[df[\"code\"] == code,\"group_id\"]=group_id\n",
    "         df.loc[df[\"code\"] == code,\"code_id\"]=code_id\n",
    "         df.loc[df[\"code\"] == code,\"code_2d_0\"]=embeddings_2d[code_id,0]\n",
    "         df.loc[df[\"code\"] == code,\"code_2d_1\"]=embeddings_2d[code_id,1]\n",
    "\n",
    "    #Update the various group stats and descriptors\n",
    "    df[\"group_codes\"]=\"\"\n",
    "    df[\"group_count\"]=0\n",
    "    df[\"group_desc\"]=\"\"\n",
    "    group_embeddings=np.zeros([n_groups,embed_dim])\n",
    "    for i in range(n_groups):\n",
    "        #update group codes and count\n",
    "        rows=df[\"group_id\"]==i\n",
    "        df.loc[rows, \"group_codes\"]=groups[i]\n",
    "        group_count=df.loc[rows,\"text_id\"].nunique()\n",
    "        group_freq=round(100.0 * group_count / n_texts)\n",
    "        df.loc[rows,\"group_count\"]=group_count\n",
    "        df.loc[rows,\"group_freq\"]=group_freq\n",
    "        df.loc[rows,\"group_code_ids\"]=\",\".join([str(code_id) for code_id in group_code_ids[i]])\n",
    "\n",
    "        #group embeddings as weighted average of code embeddings\n",
    "        code_ids=group_code_ids[i]\n",
    "        if len(code_ids)>max_group_emb_codes:\n",
    "            code_ids=code_ids[:max_group_emb_codes]\n",
    "        weights=code_counts[code_ids]\n",
    "        group_embeddings[i]=np.average(embeddings[code_ids],axis=0,weights=weights)\n",
    "        group_embeddings[i]/=np.linalg.norm(group_embeddings[i])+1e-10\n",
    "\n",
    "        #codes sorted by distance from the group embedding\n",
    "        all_code_embeddings=embeddings[group_code_ids[i]]\n",
    "        dist=np.inner(all_code_embeddings,group_embeddings[i].reshape([1,-1]))[:,0]\n",
    "        dist=1.0-dist\n",
    "        sorted_group_code_ids=np.array(group_code_ids[i])[np.argsort(dist)]\n",
    "        sorted_group_codes=np.array(codes)[sorted_group_code_ids]\n",
    "        df.loc[rows,\"sorted_group_codes\"]=\",\".join(sorted_group_codes)\n",
    "\n",
    "        # Helper for naming a group based on most frequent codes\n",
    "        def group_name_freq(codes, freq):\n",
    "            codes = codes.split(',')\n",
    "            result = codes[0]\n",
    "            if len(codes) > 1:\n",
    "                for code in codes[1:min([len(codes), group_desc_codes])]:\n",
    "                    result += \", \" + code\n",
    "            if group_desc_freq:\n",
    "                result+=f\" ({freq}%)\"\n",
    "            return result\n",
    "        df.loc[rows, \"group_desc\"]=group_name_freq(groups[i],group_freq)\n",
    "\n",
    "    #Mark outliers if using hdbscan\n",
    "    outliers=None\n",
    "    df[\"outlier\"]=0\n",
    "    df[\"not_outlier\"]=1 #just to make sorting easier\n",
    "    if clustering_method==\"hdbscan\" and has_outliers:\n",
    "        df.loc[df[\"group_id\"]==0,\"outlier\"]=1\n",
    "        df[\"not_outlier\"] = 1-df[\"outlier\"]\n",
    "        '''\n",
    "        outliers=df[df[\"group_id\"]==0]\n",
    "        df=df[df[\"group_id\"]!=0]\n",
    "        df[\"group_id\"]=df[\"group_id\"]-1\n",
    "        group_code_ids=group_code_ids[1:]\n",
    "        group_embeddings=group_embeddings[1:]\n",
    "        n_groups-=1\n",
    "        '''\n",
    "\n",
    "    #Sort\n",
    "    df.sort_values(by=[\"not_outlier\",\"group_count\",\"group_codes\",\"count\", \"code\"], inplace=True, ascending=False)\n",
    "\n",
    "    return {\"df\":df,\"embeddings\":embeddings,\"embeddings_reduced\":embeddings_reduced,\"embeddings_2d\":embeddings_2d,\"group_embeddings\":group_embeddings,\"group_code_ids\":group_code_ids,\"n_groups\":n_groups,\"code_counts\":code_counts}\n",
    "\n",
    "\n",
    "def code_df(df,\n",
    "           column_to_code,\n",
    "           coding_model,\n",
    "           embedding_model,\n",
    "           embedding_context,\n",
    "           dimred_method,\n",
    "           dimred_neighbors,\n",
    "           use_cache=True,\n",
    "           verbose=False,\n",
    "           pruned_code=None):\n",
    "    #Code\n",
    "    print(f\"Coding the {column_to_code} column of the dataframe\")\n",
    "    texts = df[column_to_code].astype(str).tolist()\n",
    "    few_shot_codes = df.loc[df['use_as_example'] == 1, 'human_codes'].astype(str).tolist()\n",
    "    few_shot_texts = df.loc[df['use_as_example'] == 1, column_to_code].astype(str).tolist()\n",
    "\n",
    "    # check if the df specifies the instructions\n",
    "    if \"coding_instructions\" in df:\n",
    "        coding_instruction = df[\"coding_instructions\"][0]\n",
    "    else:\n",
    "        raise Exception(\n",
    "            \"Coding instructions not specified. Please specify using the first row of a column named \\\"coding_instructions\\\"\")\n",
    "\n",
    "    codes = code_texts(coding_instruction=coding_instruction,\n",
    "                       few_shot_texts=few_shot_texts,\n",
    "                       few_shot_codes=few_shot_codes,\n",
    "                       gpt_model=coding_model,\n",
    "                       texts=texts,\n",
    "                       use_cache=use_cache,\n",
    "                       verbose=verbose)\n",
    "    df_coded=df.copy()\n",
    "    df_coded[\"codes\"]=codes\n",
    "    df_coded[\"text_id\"]=list(range(0, len(texts)))  #for keeping track of which original texts the codes refer to\n",
    "\n",
    "    print(\"\\nCoding instruction:\\n\\n\")\n",
    "    print(coding_instruction)\n",
    "\n",
    "    print(\"Coded data:\")\n",
    "    df_coded.reset_index(drop=True,inplace=True) #for cleaner printouts\n",
    "    print(df_coded[[column_to_code, \"codes\"]].head(20))\n",
    "\n",
    "    # Compare gpt and human codes\n",
    "    print(\"Comparing LLM and human codes based on modified Hausdorff distance in embedding space...\")\n",
    "    df_cmp=df_coded[df_coded[\"human_codes\"].notnull()].copy()\n",
    "    df_cmp=df_cmp[df_cmp[\"human_codes\"]!=\"\"]\n",
    "    df_cmp=df_cmp[df_cmp[\"use_as_example\"]!=\"1\"]\n",
    "    df_cmp=df_cmp[[column_to_code,\"human_codes\",\"codes\"]]\n",
    "    df_cmp=gpt_human_code_dist(df=df_cmp,embedding_context=embedding_context,embedding_model=embedding_model)\n",
    "\n",
    "    # Create a df with a line per code, sorted by code frequencies, with original texts/groundings in other columns\n",
    "    print(\"Formatting output...\")\n",
    "    # First, construct a df with a line code, duplicated for each coded text\n",
    "    df_codes = df_coded.copy()\n",
    "    df_codes[\"codes\"] = df_codes[\"codes\"].astype(str)\n",
    "    df_codes = extract_single_codes(df_codes)\n",
    "    df_codes = df_codes[[\"code\",\"count\",\"text_id\",column_to_code]]\n",
    "    max_count=df_codes[\"count\"].max()\n",
    "\n",
    "    # Now, pack the duplicate rows into columns, and convert from single text id:s to lists of text ids for each code\n",
    "    df_single = df_codes.drop_duplicates(subset=[\"code\"]).copy()\n",
    "    text_ids_all=[]\n",
    "    for i in range(max_count):\n",
    "        df_single[f\"text {i}\"]=None #placeholder for the texts\n",
    "\n",
    "    for code in df_single[\"code\"]:\n",
    "        rows=df_codes[df_codes[\"code\"]==code]\n",
    "        text_ids=rows[\"text_id\"].astype(str).to_list()\n",
    "        texts=rows[column_to_code].astype(str).to_list()\n",
    "        for i in range(len(texts)):\n",
    "            df_single.loc[df_single[\"code\"] == code, f\"text {i}\"]=f\"{text_ids[i]}: {texts[i]}\"\n",
    "        text_ids_all.append(\",\".join(text_ids))\n",
    "    df_single[\"text_ids\"]=text_ids_all\n",
    "    df_single=df_single.drop(columns=[\"text_id\",column_to_code])\n",
    "    df_single.reset_index(drop=True,inplace=True)\n",
    "    df_codes=df_single\n",
    "\n",
    "    # Create and print a summary of codes\n",
    "    df_code_summary=df_codes[[\"code\",\"count\",\"text 0\"]].rename(columns={\"text 0\":\"example text\"})\n",
    "    print(\"Total number of codes: \",df_single.shape[0])\n",
    "    print(\"\\nCodes sorted by number of groundings:\")\n",
    "    print(df_code_summary.head(df_code_summary.shape[0]))\n",
    "\n",
    "    # Calculate 2d embeddings for each code, for visualizing\n",
    "    print(\"Embedding codes and reducing dimensionality for visualization...\")\n",
    "    codes_with_context=[code + embedding_context for code in df_codes[\"code\"].astype(str).tolist()]\n",
    "    embeddings=embed(codes_with_context,model=embedding_model,use_cache=use_cache)\n",
    "    embeddings_2d = reduce_embedding_dimensionality(embeddings=embeddings,\n",
    "                                                    num_dimensions=2,\n",
    "                                                    use_cache=use_cache,\n",
    "                                                    method=dimred_method,\n",
    "                                                    n_neighbors=dimred_neighbors)\n",
    "    df_codes[\"code_2d_0\"]=embeddings_2d[:,0]\n",
    "    df_codes[\"code_2d_1\"]=embeddings_2d[:,1]\n",
    "\n",
    "\n",
    "    # Prune codes, if a pruning code specified\n",
    "    df_coded_pruned=None\n",
    "    df_codes_pruned=None\n",
    "    if pruned_code is not None:\n",
    "        df_coded_pruned=df_coded[~df_coded[\"codes\"].str.contains(pruned_code)].copy()\n",
    "        df_coded_pruned.reset_index(drop=True, inplace=True)\n",
    "        df_codes_pruned=df_codes[~df_codes[\"code\"].str.contains(pruned_code)].copy()\n",
    "        df_codes_pruned.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Return results as a dict\n",
    "    result={}\n",
    "    result[\"prompt\"]=construct_prompt(\n",
    "        coding_instruction=coding_instruction,\n",
    "        few_shot_texts=few_shot_texts,\n",
    "        few_shot_codes=few_shot_codes,\n",
    "        codes_so_far=None,\n",
    "        text=texts[0]\n",
    "    )\n",
    "    result[\"df_coded\"]=df_coded\n",
    "    result[\"df_codes\"]=df_codes\n",
    "    result[\"df_coded_pruned\"]=df_coded_pruned\n",
    "    result[\"df_codes_pruned\"]=df_codes_pruned\n",
    "    result[\"df_validate\"]=df_cmp\n",
    "    #result[\"df_code_summary\"]=df_code_summary\n",
    "    return result\n",
    "\n",
    "def group_codes(\n",
    "    df_codes,\n",
    "    df_data,\n",
    "    grouping_model,\n",
    "    use_cache,\n",
    "    random_seed=None,\n",
    "    verbose=False):\n",
    "\n",
    "    #extract and check instructions\n",
    "    theme_elicitation_instructions=df_data[\"theme_elicitation_instructions\"][0]\n",
    "    if not \"<codes>\" in theme_elicitation_instructions:\n",
    "        raise Exception(\"Theme elicitation instructions missing the <codes> placeholder which will be replaced with the list of codes in the LLM prompt.\")\n",
    "\n",
    "    code_grouping_instructions=df_data[\"code_grouping_instructions\"][0]\n",
    "    if not \"<codes>\" in code_grouping_instructions:\n",
    "        raise Exception(\"Code grouping instructions missing the <codes> placeholder which will be replaced with the list of codes in the LLM prompt.\")\n",
    "    if not \"<themes>\" in code_grouping_instructions:\n",
    "        raise Exception(\"Code grouping instructions missing the <themes> placeholder which will be replaced with the list of themes in the LLM prompt.\")\n",
    "\n",
    "    #STEP 1: Elicit a list of themes\n",
    "    #If there are more codes than fit a single prompt, we elicit the themes based on a random subset.\n",
    "    codes=df_codes[\"code\"].astype(str).to_list()\n",
    "    random.seed(random_seed)\n",
    "    random.shuffle(codes)\n",
    "\n",
    "    #Determine how many codes we can fit the prompt\n",
    "    code_tokens=[num_tokens_from_string(code,grouping_model) for code in codes]\n",
    "    overhead=num_tokens_from_string(string=theme_elicitation_instructions,model=grouping_model)\\\n",
    "             +token_overhead(grouping_model)+1000 #token overhead for theme list, system message and prompt start\n",
    "    max_tokens_for_codes=max_llm_context_length[grouping_model]-overhead\n",
    "    code_tokens_cumulative=0\n",
    "    max_codes=0\n",
    "    for i in range(len(code_tokens)):\n",
    "        code_tokens_cumulative+=code_tokens[i]+1 #+1 because of newlines after each code in the prompt\n",
    "        if code_tokens_cumulative>max_tokens_for_codes:\n",
    "            break\n",
    "        max_codes=i\n",
    "\n",
    "    #Prompt the themes\n",
    "    print(f\"Identifying themes based on {max_codes} codes...\")\n",
    "    codes_as_string=\"\\n\".join(codes[:max_codes])\n",
    "    prompt=theme_elicitation_instructions\n",
    "    prompt=prompt.replace(\"<codes>\",codes_as_string)\n",
    "    response=query_LLM(model=grouping_model,\n",
    "                       prompts=[prompt],\n",
    "                       temperature=0,\n",
    "                       use_cache=use_cache)[0]\n",
    "    if verbose:\n",
    "        print(\"Prompt:\\n\")\n",
    "        print(prompt+\"\\n\")\n",
    "\n",
    "    #Parse response\n",
    "    response=response[response.find(\"{\"):response.find(\"}\")+1] #extract the .json part\n",
    "    def correct_gpt_quotation_escape(s):\n",
    "        return s #s.replace('\\\\\\\"\\\"','\\\\\\\"')       #Aalto gpt-3.5-turbo sometimes does a \"double\" escape of quotation marks\n",
    "    response=correct_gpt_quotation_escape(response)\n",
    "    j = json.loads(response)\n",
    "    theme_list=j[\"themes\"]\n",
    "    theme_list=list(set(theme_list))  #remove possible duplicates\n",
    "    other_theme=\"Other\" #the theme identification probably misses some minor themes => they will get grouped under this theme\n",
    "    theme_list.append(other_theme)\n",
    "    themes_as_string=\"\\n\".join(theme_list)\n",
    "    print(\"Themes identified:\\n\")\n",
    "    print(themes_as_string+\"\\n\")\n",
    "\n",
    "    #STEP 2: Group codes under the themes in batches\n",
    "    #prompt the LLM\n",
    "    batch_size=50\n",
    "    remaining_codes=codes.copy()\n",
    "    themes={}   #results will be added to this dict, with codes as keys\n",
    "    while len(remaining_codes)>0:\n",
    "        current_remaining=len(remaining_codes)\n",
    "        prompt=code_grouping_instructions\n",
    "        code_batch=remaining_codes[:min(batch_size,len(remaining_codes))]\n",
    "        print(f\"Grouping a batch of {len(code_batch)} codes, total {current_remaining} remaining...\")\n",
    "        codes_as_string=\"\\n\".join(code_batch)\n",
    "        if verbose:\n",
    "            print(\"Code batch:\")\n",
    "            print(codes_as_string)\n",
    "        prompt=prompt.replace(\"<codes>\",codes_as_string)\n",
    "        prompt=prompt.replace(\"<themes>\",themes_as_string)\n",
    "        response=query_LLM(model=grouping_model,\n",
    "                           prompts=[prompt],\n",
    "                           use_cache=use_cache)[0]\n",
    "        response=response[response.find(\"{\"):response.find(\"}\")+1] #extract the .json part\n",
    "        response = correct_gpt_quotation_escape(response)\n",
    "        try:\n",
    "            j = json.loads(response)\n",
    "        except json.decoder.JSONDecodeError:\n",
    "            print(\".json parse of the following response failed:\\n\")\n",
    "            print(response)\n",
    "            exit()\n",
    "        #remove codes that were successfully grouped\n",
    "        for code in code_batch:\n",
    "            if code in j:\n",
    "                remaining_codes.remove(code)\n",
    "                themes[code]=j[code]\n",
    "\n",
    "        #prevent getting stuck in an infinite loop in case the LLM keeps making mistakes\n",
    "        if len(remaining_codes)==current_remaining:\n",
    "            print(\"These remaining codes could not be grouped, categorizing them as 'other':\")\n",
    "            for code in remaining_codes:\n",
    "                print(code)\n",
    "                themes[code]=\"Other\"\n",
    "            break\n",
    "\n",
    "    #Add themes to the codes dataframe\n",
    "    df_grouped=df_codes.copy()\n",
    "    df_grouped.reset_index(drop=True, inplace=True)\n",
    "    codes_all=df_grouped[\"code\"]\n",
    "    themes_all=[themes[code] for code in codes_all]\n",
    "    df_grouped[\"theme\"]=themes_all\n",
    "\n",
    "    #Calculate theme counts\n",
    "    themes_list=df_grouped[\"theme\"].unique()\n",
    "    df_grouped[\"theme_count\"]=0\n",
    "    df_grouped[\"theme_index\"]=0\n",
    "    for i,theme in enumerate(themes_list):\n",
    "        df_theme=df_grouped[df_grouped[\"theme\"]==theme]\n",
    "        df_grouped.loc[df_grouped[\"theme\"]==theme,\"theme_count\"]=df_theme[\"count\"].sum()\n",
    "        df_grouped.loc[df_grouped[\"theme\"] == theme, \"theme_index\"]=i\n",
    "\n",
    "    #Sort by both theme counts, themes, code_counts\n",
    "    df_grouped.sort_values(by=[\"theme_count\", \"theme\",\"count\"], inplace=True, ascending=False)\n",
    "    df_grouped.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    '''\n",
    "    #Replace many very small themes with \"Other\"\n",
    "    if other_threshold is not None:\n",
    "        df_other=df_grouped[df_grouped[\"theme_count\"]<=other_threshold].copy()\n",
    "        df_rest=df_grouped[df_grouped[\"theme_count\"]>other_threshold].copy()\n",
    "        df_other[\"theme\"]=\"Other\"\n",
    "        df_other[\"theme_count\"]=df_other[\"count\"].sum()\n",
    "        df_grouped=pd.concat([df_rest,df_other],axis=0)\n",
    "    '''\n",
    "\n",
    "    #Some final formatting\n",
    "    df_grouped.rename(columns={\"count\":\"code_count\"},inplace=True)\n",
    "    cols=list(df_grouped.columns.values)\n",
    "    cols.remove(\"theme\")\n",
    "    cols.remove(\"theme_count\")\n",
    "    if \"index\" in cols:\n",
    "        cols.remove(\"index\")\n",
    "    if \"level_0\" in cols:\n",
    "        cols.remove(\"level_0\")\n",
    "    df_grouped=df_grouped[[\"theme\",\"theme_count\"]+cols]\n",
    "    return df_grouped\n",
    "\n",
    "def code_and_group(df,\n",
    "                   column_to_code,\n",
    "                   coding_model,\n",
    "                   embedding_model,\n",
    "                   embedding_context=None,\n",
    "                   min_group_size=3,\n",
    "                   grouping_dim=5,\n",
    "                   use_cache=True,\n",
    "                   verbose=False,\n",
    "                   dimred_method=None,\n",
    "                   pruned_code=None):\n",
    "    if embedding_context is None:\n",
    "        embedding_context=\"\"\n",
    "\n",
    "\n",
    "\n",
    "    # Group codes\n",
    "    group_info = group_codes(df_coded,\n",
    "                             embedding_context=embedding_context,\n",
    "                             embedding_model=embedding_model,\n",
    "                             min_group_size=min_group_size,\n",
    "                             use_cache=use_cache,\n",
    "                             group_desc_codes=3,\n",
    "                             grouping_dim=grouping_dim,\n",
    "                             group_desc_freq=False,\n",
    "                             dimred_method=dimred_method)\n",
    "\n",
    "    # Create and print a summary of groups\n",
    "    df_grouped=group_info[\"df\"].copy()\n",
    "    df_grouped = df_grouped.drop_duplicates(subset=[\"group_codes\"])  # only print one text per group\n",
    "    df_grouped.reset_index(drop=True,inplace=True) #for cleaner printouts\n",
    "    df_group_summary=df_grouped[[column_to_code, \"group_desc\",\"group_freq\"]].copy()\n",
    "    print(\"\\nCode groups sorted by group frequency\")\n",
    "    print(df_group_summary.head(20))\n",
    "\n",
    "    # Convert the text-per-row grouped df to code-per-row,\n",
    "    # which allows easier editing of the results in a spreadsheet editor\n",
    "\n",
    "    # First, create an empty dataframe with correct columns\n",
    "    columns=[\"group_desc\",\"code\"]\n",
    "    for i in range(group_info[\"df\"][\"count\"].max()):\n",
    "        columns.append(f\"text_{i+1}\")\n",
    "    df_editable=None #pd.DataFrame(columns=columns)\n",
    "\n",
    "    # loop over groups\n",
    "    groups = group_info[\"df\"][\"group_id\"].unique()\n",
    "    for group_id in groups:\n",
    "        # select this group's rows\n",
    "        group_data=group_info[\"df\"][group_info[\"df\"][\"group_id\"]==group_id]\n",
    "        # skip if this is the outlier group produced by HDBSCAN\n",
    "        if group_data[\"outlier\"].iloc[0]==0:\n",
    "            # loop over codes\n",
    "            codes=group_data[\"code\"].unique()\n",
    "            for c in codes:\n",
    "                code_data=group_data[group_data[\"code\"]==c]\n",
    "                # add a line for the code\n",
    "                new_row={}\n",
    "                new_row[\"group_desc\"]=code_data[\"group_desc\"].iloc[0]\n",
    "                new_row[\"group_count\"]=code_data[\"group_count\"].iloc[0]\n",
    "                new_row[\"code\"]=c\n",
    "                new_row[\"code_count\"]=int(code_data[\"count\"].iloc[0])\n",
    "                new_row[\"code_2d_0\"]=code_data[\"code_2d_0\"].iloc[0]\n",
    "                new_row[\"code_2d_1\"] = code_data[\"code_2d_1\"].iloc[0]\n",
    "\n",
    "                for c in range(code_data.shape[0]):\n",
    "                    new_row[f\"text_{c+1}\"]=code_data[column_to_code].iloc[c]\n",
    "                new_row=pd.Series(data=new_row).to_frame().T\n",
    "                if df_editable is None:\n",
    "                    df_editable=new_row\n",
    "                else:\n",
    "                    df_editable=pd.concat(objs=[df_editable,new_row],axis=0,ignore_index=True)\n",
    "    df_editable.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # Return a dict with all the info the caller might need for further analysis or data export\n",
    "    result=group_info.copy()\n",
    "    result[\"prompt\"]=construct_prompt(\n",
    "        coding_instruction=coding_instruction,\n",
    "        few_shot_texts=few_shot_texts,\n",
    "        few_shot_codes=few_shot_codes,\n",
    "        codes_so_far=None,\n",
    "        text=texts[0]\n",
    "    )\n",
    "    result[\"df_coded\"]=df_coded_not_pruned\n",
    "    result[\"df_validate\"]=df_cmp\n",
    "    result[\"df_editable\"]=df_editable\n",
    "    result[\"df_code_summary\"]=df_code_summary\n",
    "    result[\"df_group_summary\"]=df_group_summary\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def code_and_embed(prompt_base,df,coding_model=\"text-curie-001\",embedding_model=\"text-similarity-curie-001\",column_to_code=\"texts\",embedding_context=\"\",use_cache=True,verbose=True):\n",
    "    df_coded = code(prompt_base=prompt_base,\n",
    "                      gpt_model=coding_model,\n",
    "                      df=df,\n",
    "                      column_to_code=column_to_code,\n",
    "                      use_cache=use_cache,\n",
    "                      verbose=verbose)\n",
    "    df_single = extract_single_codes(df_coded)\n",
    "    codes = df_single[\"code\"].unique().tolist()\n",
    "    code_counts=df_single.drop_duplicates(subset=[\"code\"])[\"count\"].to_numpy()\n",
    "    codes_with_context=[code + embedding_context for code in codes]\n",
    "    embeddings=embed(codes_with_context,model=embedding_model,use_cache=use_cache)\n",
    "    return {\"codes\":codes,\"counts\":code_counts,\"embeddings\":embeddings}\n",
    "\n",
    "\n",
    "def code_inductively(texts,\n",
    "                     research_question,\n",
    "                     few_shot_examples,\n",
    "                     gpt_model,\n",
    "                     use_cache=True,\n",
    "                     max_tokens=None,\n",
    "                     verbose=False):\n",
    "    if max_tokens is None:\n",
    "        # Set max_tokens dynamically based on maximum text length\n",
    "        # Note that len(text) is in characters, not tokens\n",
    "        max_tokens = max(max(len(text) for text in texts), 300)\n",
    "    \n",
    "    # Code batches of texts in parallel\n",
    "    prompts = [construct_inductive_prompt(text, research_question, few_shot_examples)\n",
    "               for text in texts]\n",
    "\n",
    "    # Query the LLM\n",
    "    continuations = query_LLM(\n",
    "        model=gpt_model,\n",
    "        prompts=prompts,\n",
    "        max_tokens=max_tokens,\n",
    "        use_cache=use_cache\n",
    "    )\n",
    "\n",
    "    # Attempt to correct any LLM formatting errors\n",
    "    coded_texts = correct_coding_errors(texts, continuations, verbose=verbose)\n",
    "\n",
    "    return coded_texts\n",
    "\n",
    "\n",
    "def code_inductively_with_code_consistency(texts,\n",
    "                                           research_question,\n",
    "                                           few_shot_examples,\n",
    "                                           gpt_model,\n",
    "                                           use_cache=True,\n",
    "                                           max_tokens=None,\n",
    "                                           verbose=False):\n",
    "    if max_tokens is None:\n",
    "        # Set max_tokens dynamically based on maximum text length\n",
    "        # Note that len(text) is in characters, not tokens\n",
    "        max_tokens = max(max(len(text) for text in texts), 300)\n",
    "\n",
    "    # Process texts sequentially to enforce code consistency\n",
    "    coded_texts = []\n",
    "    code_descriptions = {}\n",
    "    for idx, text in enumerate(texts):\n",
    "        # Update progress\n",
    "        print_progress_bar(idx + 1, len(texts), printEnd=\"\")\n",
    "\n",
    "        # Find insights independent of existing codes\n",
    "        insight_prompt = construct_insight_prompt(text, research_question)\n",
    "        print(\"\\n\\ninsight_prompt:\\n\", insight_prompt)\n",
    "        \n",
    "        continuations = query_LLM(\n",
    "            model=gpt_model,\n",
    "            prompts=[insight_prompt],\n",
    "            max_tokens=200,\n",
    "            use_cache=use_cache\n",
    "        )\n",
    "        print(\"\\n\\ncontinuations:\\n\", continuations)\n",
    "\n",
    "        insights = continuations[0]\n",
    "\n",
    "        # Construct prompt, including a list of existing codes\n",
    "        prompt = construct_inductive_prompt(\n",
    "            text=text, \n",
    "            research_question=research_question,\n",
    "            few_shot_examples=few_shot_examples,\n",
    "            code_descriptions=code_descriptions,\n",
    "            insights=insights\n",
    "        )\n",
    "        print(\"\\n\\prompt:\\n\", prompt)\n",
    "\n",
    "        # Query the LLM\n",
    "        continuations = query_LLM(\n",
    "            model=gpt_model,\n",
    "            prompts=[prompt],\n",
    "            max_tokens=max_tokens,\n",
    "            use_cache=use_cache\n",
    "        )\n",
    "        \n",
    "        # Attempt to correct any LLM formatting errors\n",
    "        coded_text_batch = correct_coding_errors([text], continuations, verbose=verbose)\n",
    "\n",
    "        # Add singular coded text to output\n",
    "        assert len(coded_text_batch) == 1\n",
    "        coded_text = coded_text_batch[0]\n",
    "        coded_texts.append(coded_text)\n",
    "\n",
    "        # For any new codes, generate description and store\n",
    "        for highlight, code in parse_codes(coded_text):\n",
    "            if code not in code_descriptions:\n",
    "                code_descriptions[code] = generate_code_description(\n",
    "                    code,\n",
    "                    [highlight],\n",
    "                    research_question,\n",
    "                    gpt_model,\n",
    "                    use_cache\n",
    "                )\n",
    "                \n",
    "    return coded_texts, code_descriptions\n",
    "\n",
    "\n",
    "def code_deductively(texts,\n",
    "                     research_question,\n",
    "                     codebook,\n",
    "                     gpt_model,\n",
    "                     few_shot_examples=None,\n",
    "                     use_cache=True,\n",
    "                     verbose=False):\n",
    "    \"\"\"\n",
    "    Apply deductive qualitative coding to a list of texts using a language model.\n",
    "\n",
    "    This function uses a predefined codebook and prompts a GPT-based model to \n",
    "    generate coded versions of the input texts. It processes the texts deductively, \n",
    "    meaning the coding is guided by the provided codebook. Optionally, few-shot \n",
    "    examples can be included to guide the model's responses. The function returns \n",
    "    the coded texts and a dictionary of codes with corresponding highlights.\n",
    "\n",
    "    Args:\n",
    "        texts (list of str): A list of texts to be coded.\n",
    "        research_question (str): The research question used to guide the language model.\n",
    "        codebook (list of str or tuple): A list of code strings or (code, description) tuples representing the codebook.\n",
    "        gpt_model (str): The identifier for the GPT model to be used for coding.\n",
    "        few_shot_examples (pd.DataFrame, optional): DataFrame containing few-shot examples to guide the model. \n",
    "            Each row should contain a 'coded_text' column with pre-coded examples.\n",
    "        use_cache (bool, optional): Whether to use cached model results if available. Defaults to True.\n",
    "        verbose (bool, optional): Whether to print warnings for coding inconsistencies and errors. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        tuple: \n",
    "            - coded_texts (list of str): The input texts with codes inserted by the language model.\n",
    "    \"\"\"\n",
    "    # Set max_tokens dynamically based on maximum text length\n",
    "    # Note that max_text_len is in characters, not tokens\n",
    "    max_text_len = max(len(text) for text in texts)\n",
    "\n",
    "    # Make codebook into a list of tuples if list of strings\n",
    "    codebook = [(item,) if isinstance(item, str) else item for item in codebook]\n",
    "\n",
    "    # Make sure codes in few_shot_examples are from codebook\n",
    "    codebook_codes = [item[0] for item in codebook]\n",
    "    if few_shot_examples is not None:\n",
    "        for _, row in few_shot_examples.iterrows():\n",
    "            for _, code in parse_codes(row.coded_text):\n",
    "                if code not in codebook_codes:\n",
    "                    print(f\"WARNING: Few-shot examples contain code \\\"{code}\\\" that is not in the codebook\")\n",
    "\n",
    "    # Query the LLM\n",
    "    prompts = [construct_deductive_prompt(text, research_question, codebook, few_shot_examples)\n",
    "               for text in texts]\n",
    "    continuations = query_LLM(\n",
    "        model=gpt_model,\n",
    "        prompts=prompts,\n",
    "        max_tokens=max_text_len,\n",
    "        use_cache=use_cache\n",
    "    )\n",
    "\n",
    "    # Attempt to correct any LLM formatting errors\n",
    "    coded_texts = correct_coding_errors(texts, continuations, verbose=verbose)\n",
    "\n",
    "    # Check output for any hallucinated codes\n",
    "    for idx in range(len(coded_texts)):\n",
    "        hallucinated_codes = []\n",
    "        for _, code in parse_codes(coded_texts[idx]):\n",
    "            if code not in codebook_codes:\n",
    "                hallucinated_codes.append(code)\n",
    "                if verbose:\n",
    "                    print(f\"WARNING: Output contains code \\\"{code}\\\" that is not in the codebook, removing...\")\n",
    "\n",
    "        # Remove any hallucinated codes\n",
    "        for code in hallucinated_codes:\n",
    "            coded_texts[idx] = _remove_code(coded_texts[idx], code)\n",
    "\n",
    "    return coded_texts\n",
    "\n",
    "\n",
    "def _remove_code(text, code_to_remove):\n",
    "    highlight_pattern = r\"(\\*\\*(.*?)\\*\\*<sup>(.*?)<\\/sup>)\"\n",
    "    for full_match, highlight, codes in re.findall(highlight_pattern, text):\n",
    "        codes = [code.strip() for code in codes.split(\";\")]\n",
    "        if code_to_remove in codes:\n",
    "            if len(codes) == 1:\n",
    "                # If it's the only code, remove the entire highlighted structure\n",
    "                new_text = highlight\n",
    "            else:\n",
    "                # Remove the specified code and reformat\n",
    "                codes = [code for code in codes if code != code_to_remove]\n",
    "                new_text = \"**{}**<sup>{}</sup>\".format(highlight, \"; \".join(codes))\n",
    "            \n",
    "            # Replace the first occurrence of the full match with the modified text\n",
    "            text = text.replace(full_match, new_text, 1)\n",
    "            break  # Only remove one occurrence per function call\n",
    "    return text\n",
    "\n",
    "\n",
    "def construct_inductive_prompt(text,\n",
    "                               research_question,\n",
    "                               few_shot_examples,\n",
    "                               code_descriptions=None,\n",
    "                               insights=None,\n",
    "                               ):\n",
    "    # Optionally add guidance about given main insights into the prompt\n",
    "    insight_text = \" and the main insights from the text\" if insights is not None else \"\"\n",
    "\n",
    "    prompt = f\"You are an expert qualitative researcher conducting a project with the research question: {research_question}. \"\n",
    "    prompt = f\"\"\"You are given a text to code inductively{insight_text}. Please carry out the following task:\n",
    "- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\n",
    "- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\n",
    "- Ignore text that is not insightful with regards to the research question.\n",
    "- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\n\"\"\"\n",
    "\n",
    "    # Optionally add existing codes into the prompt, to encourage consistency\n",
    "    if code_descriptions is not None and len(code_descriptions) > 0:\n",
    "        prompt += \"Some examples of codes in the format \\\"{code}: {description}\\\". Please create new codes when needed:\\n\"\n",
    "        # Shuffle codes to mitigate LLM recency bias\n",
    "        code_desc_str = [f\"{code}: {description}\" for code, description in code_descriptions.items()]\n",
    "        random.shuffle(code_desc_str)\n",
    "        # Add each code as a new line\n",
    "        prompt += \"\\n\".join(code_desc_str) + \"\\n\\n\"\n",
    "\n",
    "    prompt += \"Below, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\n\"\n",
    "\n",
    "    # Add the few-shot examples in random order\n",
    "    for _, row in few_shot_examples.sample(frac=1).iterrows():\n",
    "        prompt += f\"EXAMPLE INPUT:\\n{row.text}\\n\\n\"\n",
    "        prompt += f\"EXAMPLE OUTPUT:\\n{row.coded_text}\\n\\n\"\n",
    "\n",
    "    prompt += f\"ACTUAL INPUT:\\n{text}\"\n",
    "\n",
    "    if insights is not None:\n",
    "        prompt += f\"\\n\\nMAIN INSIGHTS:\\n{insights}\"\n",
    "\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def construct_deductive_prompt(text,\n",
    "                               research_question,\n",
    "                               codebook,\n",
    "                               few_shot_examples):\n",
    "    prompt = f\"You are an expert qualitative researcher conducting a project with the research question: {research_question}. \"\n",
    "    prompt += \"\"\"You are given a text to code deductively using a list of codes. Please carry out the following task:\n",
    "- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\n",
    "- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\n",
    "- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\n\"\"\"\n",
    "\n",
    "    if few_shot_examples is None:\n",
    "        prompt += \"The following is an example of the correct output format, with fictional codes:\\n\\n\"\n",
    "        prompt += \"I really enjoy walking in the park on weekends. **It helps me clear my mind**<sup>mental clarity</sup> and **feel more connected to nature**<sup>connection to nature</sup>. Sometimes, I take my dog with me, and we just wander around, **enjoying the fresh air**<sup>sensory enjoyment; relaxation</sup>.\\n\\n\"\n",
    "\n",
    "    prompt += \"Use codes from the following list:\\n\" \n",
    "    # Shuffle codebook to mitigate LLM recency bias\n",
    "    shuffled_codebook = codebook[:]\n",
    "    random.shuffle(shuffled_codebook)\n",
    "    # Add each code as a new line\n",
    "    for item in shuffled_codebook:\n",
    "        if len(item) == 1:\n",
    "            # No description included, add only code\n",
    "            prompt += f\"{item[0]}\\n\"\n",
    "        else:\n",
    "            prompt += f\"{item[0]}: {item[1]}\\n\"\n",
    "    prompt += \"\\n\"\n",
    "\n",
    "    if few_shot_examples is not None:\n",
    "        prompt += \"Below, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\n\"\n",
    "\n",
    "        # Add the few-shot examples in random order\n",
    "        for _, row in few_shot_examples.sample(frac=1).iterrows():\n",
    "            prompt += f\"EXAMPLE INPUT:\\n{row.text}\\n\\n\"\n",
    "            prompt += f\"EXAMPLE OUTPUT:\\n{row.coded_text}\\n\\n\"\n",
    "\n",
    "    prompt += f\"INPUT:\\n{text}\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def construct_insight_prompt(text, research_question):\n",
    "    prompt = f\"You are an expert qualitative researcher who is given the following text to analyze:\\n\\n{text}\\n\\n\"\n",
    "    prompt += f\"Output a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \\\"{research_question}\\\". \"\n",
    "    prompt += f\"If there are no relevant insights, output \\\"The text contains no insights relevant to the research question.\\\"\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "def generate_code_description(code, examples, research_question, gpt_model, use_cache, counter_examples=[]):\n",
    "    prompt = \"Write a brief but nuanced one-sentence description for the given inductive code, based on a set of texts annotated with the code\"\n",
    "\n",
    "    if len(counter_examples) > 0:\n",
    "        prompt += \" and counter-examples where the code does not apply.\\n\"\n",
    "    else:\n",
    "        prompt += \".\\n\"\n",
    "\n",
    "    prompt += \" For example, for the code \\\"overcommunication\\\", you might generate the description: Captures instances where participants discuss feeling overwhelmed by excessive communication, such as constant emails, messages, or meetings\\n\"\n",
    "    prompt += f\" Write the description in the context of a qualitative research project with the research question: {research_question}.\\n\\n\"\n",
    "\n",
    "    prompt += f\"CODE: {code}\\n\\n\"\n",
    "\n",
    "    prompt += \"CODED TEXTS SEPARATED BY \\\"***\\\":\\n\"\n",
    "    prompt += \"\\n***\\n\".join(examples)\n",
    "\n",
    "    if len(counter_examples) > 0:\n",
    "        prompt += \"\\n\\nCOUNTER-EXAMPLES SEPARATED BY \\\"***\\\":\\n\"\n",
    "        prompt += \"\\n***\\n\".join(examples)\n",
    "\n",
    "    continuations = query_LLM(\n",
    "        model=gpt_model,\n",
    "        prompts=[prompt],\n",
    "        max_tokens=200,\n",
    "        use_cache=use_cache\n",
    "    )\n",
    "    \n",
    "    return continuations[0]\n",
    "\n",
    "\n",
    "def correct_coding_errors(texts, continuations, verbose=False):\n",
    "    \"\"\"\n",
    "    Check for LLM errors and attempt to correct errors where the LLM has omitted non-coded parts of the original text.\n",
    "    Outputs a list of LLM-coded texts corresponding to texts, with None in place of outputs that could not be corrected.\n",
    "    \"\"\"\n",
    "    coded_texts = []\n",
    "    n_reconstructed = 0\n",
    "\n",
    "    for text, cont in zip(texts, continuations):\n",
    "        if len(cont) == 0:\n",
    "            # If the response is empty (possibly, due to model censorship), discard\n",
    "            print(f\"WARNING: Discarding empty LLM response for text \\\"{text}\\\"\\n\")\n",
    "            coded_texts.append(None)\n",
    "            continue\n",
    "\n",
    "        # Check if there were any clear errors or hallucinations. In other words, does the LLM response (cont) match\n",
    "        # the original text when we remove the highlight and code annotations?\n",
    "        cont_text = re.sub(r\"\\*\\*|<sup>(.*?)<\\/sup>\", \"\", cont)\n",
    "        if cont_text == text:\n",
    "            coded_texts.append(cont)\n",
    "            continue\n",
    "\n",
    "        # Sometimes, the LLM autocorrects typos even though we explicitly tell it not to.\n",
    "        # Ignore the difference if the edit distance between the original and LLM-highlighted text is small enough.\n",
    "        dist_threshold = 5\n",
    "        edit_dist = distance(text, cont_text)\n",
    "        if verbose:\n",
    "            print(f\"Warning: LLM output differs from the original text, with edit distance {edit_dist}\")\n",
    "            print(f\"Original text: \\\"{text}\\\"\")\n",
    "            print(f\"LLM output: \\\"{cont}\\\"\")\n",
    "        \n",
    "        if edit_dist < dist_threshold:\n",
    "            if verbose:\n",
    "                print(f\"Distance less than treshold {dist_threshold}, accept\\n\")\n",
    "            coded_texts.append(cont)\n",
    "            continue\n",
    "\n",
    "        # Error detected, attempt to reconstruct by finding the annotations in the original text with fuzzy matching\n",
    "        annotations = re.findall(r\"\\*\\*(.*?)\\*\\*(<sup>.*?<\\/sup>)\", cont)\n",
    "        reconstructed = text\n",
    "        reconstruction_failed = False\n",
    "        for highlight, codes in annotations:\n",
    "            match_start, match_end, ratio = find_best_match(reconstructed, highlight)\n",
    "            if ratio >= 90:\n",
    "                # Add annotation to reconstruction if found a match at sufficient similarity ratio\n",
    "                rec_annotation = \"**\" + reconstructed[match_start:match_end] + \"**\" + codes\n",
    "                reconstructed = reconstructed[:match_start] + rec_annotation + reconstructed[match_end:]\n",
    "            else:\n",
    "                if verbose:\n",
    "                    print(f\"Could not find the LLM-annotated text \\\"{highlight}\\\" in the original text \\\"{text}\\\"\")\n",
    "                reconstruction_failed = True\n",
    "                break\n",
    "\n",
    "        if reconstruction_failed:\n",
    "            if verbose:\n",
    "                print(f\"Text reconstruction failed, discard LLM response\\n\")\n",
    "            coded_texts.append(None)\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(f\"Text reconstruction successful\\n\")\n",
    "            coded_texts.append(reconstructed)\n",
    "            n_reconstructed += 1\n",
    "\n",
    "    if n_reconstructed > 0 and verbose:\n",
    "        print(f\"Had to reconstruct {n_reconstructed} texts due to LLM errors\")\n",
    "\n",
    "    n_discarded = sum(t is None for t in coded_texts)\n",
    "    if n_discarded > 0:\n",
    "        print(f\"WARNING: A total of {n_discarded} LLM outputs were discarded because of errors\\n\")\n",
    "\n",
    "    return coded_texts\n",
    "\n",
    "\n",
    "def find_best_match(original_text, target_substring):\n",
    "    \"\"\"\n",
    "    Finds the substring in original_text that best matches target_substring using\n",
    "    fuzzy matching, allowing for some length variance.\n",
    "\n",
    "    Args:\n",
    "        original_text (str): The text to search in.\n",
    "        target_substring (str): The string to find a similar match for.\n",
    "\n",
    "    Returns:\n",
    "        tuple: (best_start, best_end, best_ratio) where best_start and best_end are\n",
    "        the indices of the best matching substring, and best_ratio is the similarity score.\n",
    "    \"\"\"\n",
    "\n",
    "    best_ratio = 0\n",
    "    best_start = 0\n",
    "    best_end = 0\n",
    "\n",
    "    # We only search for matches within max_length_variance from len(target_substring)\n",
    "    max_length_variance = int(len(target_substring) / 4)\n",
    "\n",
    "    # Iterate over all possible window sizes\n",
    "    for start in range(len(original_text)):\n",
    "        end_min = start + len(target_substring) - max_length_variance\n",
    "        end_max = min(start + len(target_substring) + max_length_variance, len(original_text) + 1)\n",
    "        for end in range(end_min, end_max):\n",
    "            window = original_text[start:end]\n",
    "            similarity_ratio = fuzz.ratio(window, target_substring)\n",
    "            \n",
    "            # Update the best match if this window has a better match\n",
    "            if similarity_ratio > best_ratio:\n",
    "                best_ratio = similarity_ratio\n",
    "                best_start = start\n",
    "                best_end = end\n",
    "\n",
    "    return best_start, best_end, best_ratio\n",
    "\n",
    "\n",
    "def get_codes_and_highlights(coded_texts):\n",
    "    code_highlights = defaultdict(list)\n",
    "    for coded_text in coded_texts:\n",
    "        for highlight, code in parse_codes(coded_text):\n",
    "            code_highlights[code].append(highlight)\n",
    "    return code_highlights\n",
    "\n",
    "\n",
    "def parse_codes(coded_text):\n",
    "    if coded_text is None:\n",
    "        return []\n",
    "    return [(highlight, code.strip()) \n",
    "            for highlight, codes in re.findall(r\"\\*\\*(.*?)\\*\\*<sup>(.*?)<\\/sup>\", coded_text)\n",
    "            for code in codes.split(\";\")]\n",
    "\n",
    "\n",
    "def get_2d_code_embeddings(codes, embedding_context, embedding_model, verbose=False):\n",
    "    # Add context to each set of codes before embedding\n",
    "    codes_with_context = [code + embedding_context for code in codes]\n",
    "    embeddings = embed(codes_with_context, model=embedding_model, verbose=verbose)\n",
    "\n",
    "    # Reduce embedding dimensionality to 2\n",
    "    embeddings_2d = reduce_embedding_dimensionality(embeddings=embeddings, num_dimensions=2, verbose=verbose)\n",
    "\n",
    "    # Return results as a DataFrame\n",
    "    df_em = pd.DataFrame([(code,) for code in codes], columns=[\"code\"])\n",
    "    df_em[\"code_2d_0\"] = embeddings_2d[:,0]\n",
    "    df_em[\"code_2d_1\"] = embeddings_2d[:,1]\n",
    "    return df_em\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Assuming 'src' is one level down (in the current directory or a subdirectory)\n",
    "path_to_src = os.path.join('src')  # Moves one level down to 'src' folder\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(path_to_src)\n",
    "\n",
    "# Now you can import your API_key module\n",
    "import API_key as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = key.openAI_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "init(API=\"OpenAI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ--------------------------------------------------------------------------------| 20.0% \n",
      "\n",
      "insight_prompt:\n",
      " You are an expert qualitative researcher who is given the following text to analyze:\n",
      "\n",
      "The companyâ€™s culture promotes teamwork and employee satisfaction.\n",
      "\n",
      "Output a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"\n",
      "N: 1\n",
      "prompt_batch for loop: ['You are an expert qualitative researcher who is given the following text to analyze:\\n\\nThe companyâ€™s culture promotes teamwork and employee satisfaction.\\n\\nOutput a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"']\n",
      "Arguments:\n",
      "model: gpt-4o\n",
      "prompt_batch: ['You are an expert qualitative researcher who is given the following text to analyze:\\n\\nThe companyâ€™s culture promotes teamwork and employee satisfaction.\\n\\nOutput a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"']\n",
      "max_tokens: 200\n",
      "use_cache: True\n",
      "temperature: 0\n",
      "system_message: None\n",
      "stop: None\n",
      "OK 1\n",
      "Checking for None values in API_type, model, and prompt_batch...\n",
      "API_type: OpenAI\n",
      "model: gpt-4o\n",
      "prompt_batch element at index 0: You are an expert qualitative researcher who is given the following text to analyze:\n",
      "\n",
      "The companyâ€™s culture promotes teamwork and employee satisfaction.\n",
      "\n",
      "Output a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"\n",
      "continuations for loop: ['The text suggests that teamwork and employee satisfaction are key elements of a positive work environment.']\n",
      "\n",
      "\n",
      "continuations:\n",
      " ['The text suggests that teamwork and employee satisfaction are key elements of a positive work environment.']\n",
      "\n",
      "\\prompt:\n",
      " You are given a text to code inductively and the main insights from the text. Please carry out the following task:\n",
      "- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\n",
      "- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\n",
      "- Ignore text that is not insightful with regards to the research question.\n",
      "- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\n",
      "\n",
      "Below, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "I feel valued and appreciated by my team.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "The training programs are really beneficial.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "The **training programs**<sup>professional development</sup> are really beneficial.\n",
      "\n",
      "ACTUAL INPUT:\n",
      "The companyâ€™s culture promotes teamwork and employee satisfaction.\n",
      "\n",
      "MAIN INSIGHTS:\n",
      "The text suggests that teamwork and employee satisfaction are key elements of a positive work environment.\n",
      "N: 1\n",
      "prompt_batch for loop: ['You are given a text to code inductively and the main insights from the text. Please carry out the following task:\\n- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\\n- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\\n- Ignore text that is not insightful with regards to the research question.\\n- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\nBelow, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\nEXAMPLE INPUT:\\nI feel valued and appreciated by my team.\\n\\nEXAMPLE OUTPUT:\\n**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\\n\\nEXAMPLE INPUT:\\nThe training programs are really beneficial.\\n\\nEXAMPLE OUTPUT:\\nThe **training programs**<sup>professional development</sup> are really beneficial.\\n\\nACTUAL INPUT:\\nThe companyâ€™s culture promotes teamwork and employee satisfaction.\\n\\nMAIN INSIGHTS:\\nThe text suggests that teamwork and employee satisfaction are key elements of a positive work environment.']\n",
      "Arguments:\n",
      "model: gpt-4o\n",
      "prompt_batch: ['You are given a text to code inductively and the main insights from the text. Please carry out the following task:\\n- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\\n- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\\n- Ignore text that is not insightful with regards to the research question.\\n- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\nBelow, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\nEXAMPLE INPUT:\\nI feel valued and appreciated by my team.\\n\\nEXAMPLE OUTPUT:\\n**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\\n\\nEXAMPLE INPUT:\\nThe training programs are really beneficial.\\n\\nEXAMPLE OUTPUT:\\nThe **training programs**<sup>professional development</sup> are really beneficial.\\n\\nACTUAL INPUT:\\nThe companyâ€™s culture promotes teamwork and employee satisfaction.\\n\\nMAIN INSIGHTS:\\nThe text suggests that teamwork and employee satisfaction are key elements of a positive work environment.']\n",
      "max_tokens: 150\n",
      "use_cache: True\n",
      "temperature: 0\n",
      "system_message: None\n",
      "stop: None\n",
      "OK 1\n",
      "Checking for None values in API_type, model, and prompt_batch...\n",
      "API_type: OpenAI\n",
      "model: gpt-4o\n",
      "prompt_batch element at index 0: You are given a text to code inductively and the main insights from the text. Please carry out the following task:\n",
      "- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\n",
      "- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\n",
      "- Ignore text that is not insightful with regards to the research question.\n",
      "- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\n",
      "\n",
      "Below, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "I feel valued and appreciated by my team.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "The training programs are really beneficial.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "The **training programs**<sup>professional development</sup> are really beneficial.\n",
      "\n",
      "ACTUAL INPUT:\n",
      "The companyâ€™s culture promotes teamwork and employee satisfaction.\n",
      "\n",
      "MAIN INSIGHTS:\n",
      "The text suggests that teamwork and employee satisfaction are key elements of a positive work environment.\n",
      "OK 2\n",
      "OK 3\n",
      "continuations for loop: ['The companyâ€™s culture promotes **teamwork**<sup>positive work environment</sup> and **employee satisfaction**<sup>positive work environment</sup>.']\n",
      "N: 1\n",
      "prompt_batch for loop: ['Write a brief but nuanced one-sentence description for the given inductive code, based on a set of texts annotated with the code.\\n For example, for the code \"overcommunication\", you might generate the description: Captures instances where participants discuss feeling overwhelmed by excessive communication, such as constant emails, messages, or meetings\\n Write the description in the context of a qualitative research project with the research question: What are the key elements of a positive work environment?.\\n\\nCODE: positive work environment\\n\\nCODED TEXTS SEPARATED BY \"***\":\\nteamwork']\n",
      "Arguments:\n",
      "model: gpt-4o\n",
      "prompt_batch: ['Write a brief but nuanced one-sentence description for the given inductive code, based on a set of texts annotated with the code.\\n For example, for the code \"overcommunication\", you might generate the description: Captures instances where participants discuss feeling overwhelmed by excessive communication, such as constant emails, messages, or meetings\\n Write the description in the context of a qualitative research project with the research question: What are the key elements of a positive work environment?.\\n\\nCODE: positive work environment\\n\\nCODED TEXTS SEPARATED BY \"***\":\\nteamwork']\n",
      "max_tokens: 200\n",
      "use_cache: True\n",
      "temperature: 0\n",
      "system_message: None\n",
      "stop: None\n",
      "OK 1\n",
      "Checking for None values in API_type, model, and prompt_batch...\n",
      "API_type: OpenAI\n",
      "model: gpt-4o\n",
      "prompt_batch element at index 0: Write a brief but nuanced one-sentence description for the given inductive code, based on a set of texts annotated with the code.\n",
      " For example, for the code \"overcommunication\", you might generate the description: Captures instances where participants discuss feeling overwhelmed by excessive communication, such as constant emails, messages, or meetings\n",
      " Write the description in the context of a qualitative research project with the research question: What are the key elements of a positive work environment?.\n",
      "\n",
      "CODE: positive work environment\n",
      "\n",
      "CODED TEXTS SEPARATED BY \"***\":\n",
      "teamwork\n",
      "continuations for loop: ['Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.']\n",
      " |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ------------------------------------------------------------| 40.0% \n",
      "\n",
      "insight_prompt:\n",
      " You are an expert qualitative researcher who is given the following text to analyze:\n",
      "\n",
      "Working here offers many opportunities for growth and learning.\n",
      "\n",
      "Output a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"\n",
      "N: 1\n",
      "prompt_batch for loop: ['You are an expert qualitative researcher who is given the following text to analyze:\\n\\nWorking here offers many opportunities for growth and learning.\\n\\nOutput a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"']\n",
      "Arguments:\n",
      "model: gpt-4o\n",
      "prompt_batch: ['You are an expert qualitative researcher who is given the following text to analyze:\\n\\nWorking here offers many opportunities for growth and learning.\\n\\nOutput a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"']\n",
      "max_tokens: 200\n",
      "use_cache: True\n",
      "temperature: 0\n",
      "system_message: None\n",
      "stop: None\n",
      "OK 1\n",
      "Checking for None values in API_type, model, and prompt_batch...\n",
      "API_type: OpenAI\n",
      "model: gpt-4o\n",
      "prompt_batch element at index 0: You are an expert qualitative researcher who is given the following text to analyze:\n",
      "\n",
      "Working here offers many opportunities for growth and learning.\n",
      "\n",
      "Output a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"\n",
      "continuations for loop: ['The text contains no insights relevant to the research question.']\n",
      "\n",
      "\n",
      "continuations:\n",
      " ['The text contains no insights relevant to the research question.']\n",
      "\n",
      "\\prompt:\n",
      " You are given a text to code inductively and the main insights from the text. Please carry out the following task:\n",
      "- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\n",
      "- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\n",
      "- Ignore text that is not insightful with regards to the research question.\n",
      "- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\n",
      "\n",
      "Some examples of codes in the format \"{code}: {description}\". Please create new codes when needed:\n",
      "positive work environment: Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.\n",
      "\n",
      "Below, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "The training programs are really beneficial.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "The **training programs**<sup>professional development</sup> are really beneficial.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "I feel valued and appreciated by my team.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\n",
      "\n",
      "ACTUAL INPUT:\n",
      "Working here offers many opportunities for growth and learning.\n",
      "\n",
      "MAIN INSIGHTS:\n",
      "The text contains no insights relevant to the research question.\n",
      "N: 1\n",
      "prompt_batch for loop: ['You are given a text to code inductively and the main insights from the text. Please carry out the following task:\\n- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\\n- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\\n- Ignore text that is not insightful with regards to the research question.\\n- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\nSome examples of codes in the format \"{code}: {description}\". Please create new codes when needed:\\npositive work environment: Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.\\n\\nBelow, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\nEXAMPLE INPUT:\\nThe training programs are really beneficial.\\n\\nEXAMPLE OUTPUT:\\nThe **training programs**<sup>professional development</sup> are really beneficial.\\n\\nEXAMPLE INPUT:\\nI feel valued and appreciated by my team.\\n\\nEXAMPLE OUTPUT:\\n**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\\n\\nACTUAL INPUT:\\nWorking here offers many opportunities for growth and learning.\\n\\nMAIN INSIGHTS:\\nThe text contains no insights relevant to the research question.']\n",
      "Arguments:\n",
      "model: gpt-4o\n",
      "prompt_batch: ['You are given a text to code inductively and the main insights from the text. Please carry out the following task:\\n- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\\n- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\\n- Ignore text that is not insightful with regards to the research question.\\n- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\nSome examples of codes in the format \"{code}: {description}\". Please create new codes when needed:\\npositive work environment: Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.\\n\\nBelow, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\nEXAMPLE INPUT:\\nThe training programs are really beneficial.\\n\\nEXAMPLE OUTPUT:\\nThe **training programs**<sup>professional development</sup> are really beneficial.\\n\\nEXAMPLE INPUT:\\nI feel valued and appreciated by my team.\\n\\nEXAMPLE OUTPUT:\\n**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\\n\\nACTUAL INPUT:\\nWorking here offers many opportunities for growth and learning.\\n\\nMAIN INSIGHTS:\\nThe text contains no insights relevant to the research question.']\n",
      "max_tokens: 150\n",
      "use_cache: True\n",
      "temperature: 0\n",
      "system_message: None\n",
      "stop: None\n",
      "OK 1\n",
      "Checking for None values in API_type, model, and prompt_batch...\n",
      "API_type: OpenAI\n",
      "model: gpt-4o\n",
      "prompt_batch element at index 0: You are given a text to code inductively and the main insights from the text. Please carry out the following task:\n",
      "- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\n",
      "- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\n",
      "- Ignore text that is not insightful with regards to the research question.\n",
      "- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\n",
      "\n",
      "Some examples of codes in the format \"{code}: {description}\". Please create new codes when needed:\n",
      "positive work environment: Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.\n",
      "\n",
      "Below, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "The training programs are really beneficial.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "The **training programs**<sup>professional development</sup> are really beneficial.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "I feel valued and appreciated by my team.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\n",
      "\n",
      "ACTUAL INPUT:\n",
      "Working here offers many opportunities for growth and learning.\n",
      "\n",
      "MAIN INSIGHTS:\n",
      "The text contains no insights relevant to the research question.\n",
      "OK 2\n",
      "OK 3\n",
      "continuations for loop: [\"I'm sorry, but there are no insightful statements in the provided text that relate to the research question.\"]\n",
      "Warning: LLM output differs from the original text, with edit distance 81\n",
      "Original text: \"Working here offers many opportunities for growth and learning.\"\n",
      "LLM output: \"I'm sorry, but there are no insightful statements in the provided text that relate to the research question.\"\n",
      "Text reconstruction successful\n",
      "\n",
      "Had to reconstruct 1 texts due to LLM errors\n",
      " |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ----------------------------------------| 60.0% \n",
      "\n",
      "insight_prompt:\n",
      " You are an expert qualitative researcher who is given the following text to analyze:\n",
      "\n",
      "I am getting more creative working in such an positive environment.\n",
      "\n",
      "Output a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"\n",
      "N: 1\n",
      "prompt_batch for loop: ['You are an expert qualitative researcher who is given the following text to analyze:\\n\\nI am getting more creative working in such an positive environment.\\n\\nOutput a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"']\n",
      "Arguments:\n",
      "model: gpt-4o\n",
      "prompt_batch: ['You are an expert qualitative researcher who is given the following text to analyze:\\n\\nI am getting more creative working in such an positive environment.\\n\\nOutput a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"']\n",
      "max_tokens: 200\n",
      "use_cache: True\n",
      "temperature: 0\n",
      "system_message: None\n",
      "stop: None\n",
      "OK 1\n",
      "Checking for None values in API_type, model, and prompt_batch...\n",
      "API_type: OpenAI\n",
      "model: gpt-4o\n",
      "prompt_batch element at index 0: You are an expert qualitative researcher who is given the following text to analyze:\n",
      "\n",
      "I am getting more creative working in such an positive environment.\n",
      "\n",
      "Output a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"\n",
      "OK 2\n",
      "OK 3\n",
      "continuations for loop: ['The text suggests that a positive work environment fosters creativity.']\n",
      "\n",
      "\n",
      "continuations:\n",
      " ['The text suggests that a positive work environment fosters creativity.']\n",
      "\n",
      "\\prompt:\n",
      " You are given a text to code inductively and the main insights from the text. Please carry out the following task:\n",
      "- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\n",
      "- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\n",
      "- Ignore text that is not insightful with regards to the research question.\n",
      "- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\n",
      "\n",
      "Some examples of codes in the format \"{code}: {description}\". Please create new codes when needed:\n",
      "positive work environment: Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.\n",
      "\n",
      "Below, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "I feel valued and appreciated by my team.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "The training programs are really beneficial.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "The **training programs**<sup>professional development</sup> are really beneficial.\n",
      "\n",
      "ACTUAL INPUT:\n",
      "I am getting more creative working in such an positive environment.\n",
      "\n",
      "MAIN INSIGHTS:\n",
      "The text suggests that a positive work environment fosters creativity.\n",
      "N: 1\n",
      "prompt_batch for loop: ['You are given a text to code inductively and the main insights from the text. Please carry out the following task:\\n- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\\n- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\\n- Ignore text that is not insightful with regards to the research question.\\n- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\nSome examples of codes in the format \"{code}: {description}\". Please create new codes when needed:\\npositive work environment: Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.\\n\\nBelow, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\nEXAMPLE INPUT:\\nI feel valued and appreciated by my team.\\n\\nEXAMPLE OUTPUT:\\n**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\\n\\nEXAMPLE INPUT:\\nThe training programs are really beneficial.\\n\\nEXAMPLE OUTPUT:\\nThe **training programs**<sup>professional development</sup> are really beneficial.\\n\\nACTUAL INPUT:\\nI am getting more creative working in such an positive environment.\\n\\nMAIN INSIGHTS:\\nThe text suggests that a positive work environment fosters creativity.']\n",
      "Arguments:\n",
      "model: gpt-4o\n",
      "prompt_batch: ['You are given a text to code inductively and the main insights from the text. Please carry out the following task:\\n- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\\n- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\\n- Ignore text that is not insightful with regards to the research question.\\n- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\nSome examples of codes in the format \"{code}: {description}\". Please create new codes when needed:\\npositive work environment: Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.\\n\\nBelow, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\nEXAMPLE INPUT:\\nI feel valued and appreciated by my team.\\n\\nEXAMPLE OUTPUT:\\n**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\\n\\nEXAMPLE INPUT:\\nThe training programs are really beneficial.\\n\\nEXAMPLE OUTPUT:\\nThe **training programs**<sup>professional development</sup> are really beneficial.\\n\\nACTUAL INPUT:\\nI am getting more creative working in such an positive environment.\\n\\nMAIN INSIGHTS:\\nThe text suggests that a positive work environment fosters creativity.']\n",
      "max_tokens: 150\n",
      "use_cache: True\n",
      "temperature: 0\n",
      "system_message: None\n",
      "stop: None\n",
      "OK 1\n",
      "Checking for None values in API_type, model, and prompt_batch...\n",
      "API_type: OpenAI\n",
      "model: gpt-4o\n",
      "prompt_batch element at index 0: You are given a text to code inductively and the main insights from the text. Please carry out the following task:\n",
      "- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\n",
      "- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\n",
      "- Ignore text that is not insightful with regards to the research question.\n",
      "- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\n",
      "\n",
      "Some examples of codes in the format \"{code}: {description}\". Please create new codes when needed:\n",
      "positive work environment: Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.\n",
      "\n",
      "Below, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "I feel valued and appreciated by my team.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "The training programs are really beneficial.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "The **training programs**<sup>professional development</sup> are really beneficial.\n",
      "\n",
      "ACTUAL INPUT:\n",
      "I am getting more creative working in such an positive environment.\n",
      "\n",
      "MAIN INSIGHTS:\n",
      "The text suggests that a positive work environment fosters creativity.\n",
      "OK 2\n",
      "OK 3\n",
      "continuations for loop: ['I am getting more **creative**<sup>creativity</sup> working in such a **positive environment**<sup>positive work environment</sup>.']\n",
      "Warning: LLM output differs from the original text, with edit distance 1\n",
      "Original text: \"I am getting more creative working in such an positive environment.\"\n",
      "LLM output: \"I am getting more **creative**<sup>creativity</sup> working in such a **positive environment**<sup>positive work environment</sup>.\"\n",
      "Distance less than treshold 5, accept\n",
      "\n",
      "N: 1\n",
      "prompt_batch for loop: ['Write a brief but nuanced one-sentence description for the given inductive code, based on a set of texts annotated with the code.\\n For example, for the code \"overcommunication\", you might generate the description: Captures instances where participants discuss feeling overwhelmed by excessive communication, such as constant emails, messages, or meetings\\n Write the description in the context of a qualitative research project with the research question: What are the key elements of a positive work environment?.\\n\\nCODE: creativity\\n\\nCODED TEXTS SEPARATED BY \"***\":\\ncreative']\n",
      "Arguments:\n",
      "model: gpt-4o\n",
      "prompt_batch: ['Write a brief but nuanced one-sentence description for the given inductive code, based on a set of texts annotated with the code.\\n For example, for the code \"overcommunication\", you might generate the description: Captures instances where participants discuss feeling overwhelmed by excessive communication, such as constant emails, messages, or meetings\\n Write the description in the context of a qualitative research project with the research question: What are the key elements of a positive work environment?.\\n\\nCODE: creativity\\n\\nCODED TEXTS SEPARATED BY \"***\":\\ncreative']\n",
      "max_tokens: 200\n",
      "use_cache: True\n",
      "temperature: 0\n",
      "system_message: None\n",
      "stop: None\n",
      "OK 1\n",
      "Checking for None values in API_type, model, and prompt_batch...\n",
      "API_type: OpenAI\n",
      "model: gpt-4o\n",
      "prompt_batch element at index 0: Write a brief but nuanced one-sentence description for the given inductive code, based on a set of texts annotated with the code.\n",
      " For example, for the code \"overcommunication\", you might generate the description: Captures instances where participants discuss feeling overwhelmed by excessive communication, such as constant emails, messages, or meetings\n",
      " Write the description in the context of a qualitative research project with the research question: What are the key elements of a positive work environment?.\n",
      "\n",
      "CODE: creativity\n",
      "\n",
      "CODED TEXTS SEPARATED BY \"***\":\n",
      "creative\n",
      "OK 2\n",
      "OK 3\n",
      "continuations for loop: ['Captures instances where participants highlight the importance of an environment that encourages innovative thinking and the freedom to explore new ideas without fear of judgment or failure.']\n",
      " |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ--------------------| 80.0% \n",
      "\n",
      "insight_prompt:\n",
      " You are an expert qualitative researcher who is given the following text to analyze:\n",
      "\n",
      "The management is supportive and values employee well-being.\n",
      "\n",
      "Output a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"\n",
      "N: 1\n",
      "prompt_batch for loop: ['You are an expert qualitative researcher who is given the following text to analyze:\\n\\nThe management is supportive and values employee well-being.\\n\\nOutput a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"']\n",
      "Arguments:\n",
      "model: gpt-4o\n",
      "prompt_batch: ['You are an expert qualitative researcher who is given the following text to analyze:\\n\\nThe management is supportive and values employee well-being.\\n\\nOutput a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"']\n",
      "max_tokens: 200\n",
      "use_cache: True\n",
      "temperature: 0\n",
      "system_message: None\n",
      "stop: None\n",
      "OK 1\n",
      "Checking for None values in API_type, model, and prompt_batch...\n",
      "API_type: OpenAI\n",
      "model: gpt-4o\n",
      "prompt_batch element at index 0: You are an expert qualitative researcher who is given the following text to analyze:\n",
      "\n",
      "The management is supportive and values employee well-being.\n",
      "\n",
      "Output a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"\n",
      "OK 2\n",
      "OK 3\n",
      "continuations for loop: ['The text highlights that supportive management and valuing employee well-being are key elements of a positive work environment.']\n",
      "\n",
      "\n",
      "continuations:\n",
      " ['The text highlights that supportive management and valuing employee well-being are key elements of a positive work environment.']\n",
      "\n",
      "\\prompt:\n",
      " You are given a text to code inductively and the main insights from the text. Please carry out the following task:\n",
      "- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\n",
      "- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\n",
      "- Ignore text that is not insightful with regards to the research question.\n",
      "- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\n",
      "\n",
      "Some examples of codes in the format \"{code}: {description}\". Please create new codes when needed:\n",
      "creativity: Captures instances where participants highlight the importance of an environment that encourages innovative thinking and the freedom to explore new ideas without fear of judgment or failure.\n",
      "positive work environment: Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.\n",
      "\n",
      "Below, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "The training programs are really beneficial.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "The **training programs**<sup>professional development</sup> are really beneficial.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "I feel valued and appreciated by my team.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\n",
      "\n",
      "ACTUAL INPUT:\n",
      "The management is supportive and values employee well-being.\n",
      "\n",
      "MAIN INSIGHTS:\n",
      "The text highlights that supportive management and valuing employee well-being are key elements of a positive work environment.\n",
      "N: 1\n",
      "prompt_batch for loop: ['You are given a text to code inductively and the main insights from the text. Please carry out the following task:\\n- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\\n- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\\n- Ignore text that is not insightful with regards to the research question.\\n- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\nSome examples of codes in the format \"{code}: {description}\". Please create new codes when needed:\\ncreativity: Captures instances where participants highlight the importance of an environment that encourages innovative thinking and the freedom to explore new ideas without fear of judgment or failure.\\npositive work environment: Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.\\n\\nBelow, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\nEXAMPLE INPUT:\\nThe training programs are really beneficial.\\n\\nEXAMPLE OUTPUT:\\nThe **training programs**<sup>professional development</sup> are really beneficial.\\n\\nEXAMPLE INPUT:\\nI feel valued and appreciated by my team.\\n\\nEXAMPLE OUTPUT:\\n**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\\n\\nACTUAL INPUT:\\nThe management is supportive and values employee well-being.\\n\\nMAIN INSIGHTS:\\nThe text highlights that supportive management and valuing employee well-being are key elements of a positive work environment.']\n",
      "Arguments:\n",
      "model: gpt-4o\n",
      "prompt_batch: ['You are given a text to code inductively and the main insights from the text. Please carry out the following task:\\n- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\\n- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\\n- Ignore text that is not insightful with regards to the research question.\\n- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\nSome examples of codes in the format \"{code}: {description}\". Please create new codes when needed:\\ncreativity: Captures instances where participants highlight the importance of an environment that encourages innovative thinking and the freedom to explore new ideas without fear of judgment or failure.\\npositive work environment: Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.\\n\\nBelow, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\nEXAMPLE INPUT:\\nThe training programs are really beneficial.\\n\\nEXAMPLE OUTPUT:\\nThe **training programs**<sup>professional development</sup> are really beneficial.\\n\\nEXAMPLE INPUT:\\nI feel valued and appreciated by my team.\\n\\nEXAMPLE OUTPUT:\\n**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\\n\\nACTUAL INPUT:\\nThe management is supportive and values employee well-being.\\n\\nMAIN INSIGHTS:\\nThe text highlights that supportive management and valuing employee well-being are key elements of a positive work environment.']\n",
      "max_tokens: 150\n",
      "use_cache: True\n",
      "temperature: 0\n",
      "system_message: None\n",
      "stop: None\n",
      "OK 1\n",
      "Checking for None values in API_type, model, and prompt_batch...\n",
      "API_type: OpenAI\n",
      "model: gpt-4o\n",
      "prompt_batch element at index 0: You are given a text to code inductively and the main insights from the text. Please carry out the following task:\n",
      "- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\n",
      "- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\n",
      "- Ignore text that is not insightful with regards to the research question.\n",
      "- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\n",
      "\n",
      "Some examples of codes in the format \"{code}: {description}\". Please create new codes when needed:\n",
      "creativity: Captures instances where participants highlight the importance of an environment that encourages innovative thinking and the freedom to explore new ideas without fear of judgment or failure.\n",
      "positive work environment: Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.\n",
      "\n",
      "Below, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "The training programs are really beneficial.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "The **training programs**<sup>professional development</sup> are really beneficial.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "I feel valued and appreciated by my team.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\n",
      "\n",
      "ACTUAL INPUT:\n",
      "The management is supportive and values employee well-being.\n",
      "\n",
      "MAIN INSIGHTS:\n",
      "The text highlights that supportive management and valuing employee well-being are key elements of a positive work environment.\n",
      "OK 2\n",
      "OK 3\n",
      "continuations for loop: ['The **management is supportive and values employee well-being**<sup>positive work environment</sup>.']\n",
      " |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100.0% \n",
      "\n",
      "\n",
      "insight_prompt:\n",
      " You are an expert qualitative researcher who is given the following text to analyze:\n",
      "\n",
      "Learning and development programs are easily accessible and beneficial.\n",
      "\n",
      "Output a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"\n",
      "N: 1\n",
      "prompt_batch for loop: ['You are an expert qualitative researcher who is given the following text to analyze:\\n\\nLearning and development programs are easily accessible and beneficial.\\n\\nOutput a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"']\n",
      "Arguments:\n",
      "model: gpt-4o\n",
      "prompt_batch: ['You are an expert qualitative researcher who is given the following text to analyze:\\n\\nLearning and development programs are easily accessible and beneficial.\\n\\nOutput a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"']\n",
      "max_tokens: 200\n",
      "use_cache: True\n",
      "temperature: 0\n",
      "system_message: None\n",
      "stop: None\n",
      "OK 1\n",
      "Checking for None values in API_type, model, and prompt_batch...\n",
      "API_type: OpenAI\n",
      "model: gpt-4o\n",
      "prompt_batch element at index 0: You are an expert qualitative researcher who is given the following text to analyze:\n",
      "\n",
      "Learning and development programs are easily accessible and beneficial.\n",
      "\n",
      "Output a single sentence summarising the most interesting insights in the text, specifically pertaining to the research question \"What are the key elements of a positive work environment?\". If there are no relevant insights, output \"The text contains no insights relevant to the research question.\"\n",
      "OK 2\n",
      "OK 3\n",
      "continuations for loop: ['The text contains no insights relevant to the research question.']\n",
      "\n",
      "\n",
      "continuations:\n",
      " ['The text contains no insights relevant to the research question.']\n",
      "\n",
      "\\prompt:\n",
      " You are given a text to code inductively and the main insights from the text. Please carry out the following task:\n",
      "- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\n",
      "- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\n",
      "- Ignore text that is not insightful with regards to the research question.\n",
      "- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\n",
      "\n",
      "Some examples of codes in the format \"{code}: {description}\". Please create new codes when needed:\n",
      "creativity: Captures instances where participants highlight the importance of an environment that encourages innovative thinking and the freedom to explore new ideas without fear of judgment or failure.\n",
      "positive work environment: Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.\n",
      "\n",
      "Below, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "The training programs are really beneficial.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "The **training programs**<sup>professional development</sup> are really beneficial.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "I feel valued and appreciated by my team.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\n",
      "\n",
      "ACTUAL INPUT:\n",
      "Learning and development programs are easily accessible and beneficial.\n",
      "\n",
      "MAIN INSIGHTS:\n",
      "The text contains no insights relevant to the research question.\n",
      "N: 1\n",
      "prompt_batch for loop: ['You are given a text to code inductively and the main insights from the text. Please carry out the following task:\\n- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\\n- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\\n- Ignore text that is not insightful with regards to the research question.\\n- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\nSome examples of codes in the format \"{code}: {description}\". Please create new codes when needed:\\ncreativity: Captures instances where participants highlight the importance of an environment that encourages innovative thinking and the freedom to explore new ideas without fear of judgment or failure.\\npositive work environment: Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.\\n\\nBelow, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\nEXAMPLE INPUT:\\nThe training programs are really beneficial.\\n\\nEXAMPLE OUTPUT:\\nThe **training programs**<sup>professional development</sup> are really beneficial.\\n\\nEXAMPLE INPUT:\\nI feel valued and appreciated by my team.\\n\\nEXAMPLE OUTPUT:\\n**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\\n\\nACTUAL INPUT:\\nLearning and development programs are easily accessible and beneficial.\\n\\nMAIN INSIGHTS:\\nThe text contains no insights relevant to the research question.']\n",
      "Arguments:\n",
      "model: gpt-4o\n",
      "prompt_batch: ['You are given a text to code inductively and the main insights from the text. Please carry out the following task:\\n- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\\n- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\\n- Ignore text that is not insightful with regards to the research question.\\n- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\nSome examples of codes in the format \"{code}: {description}\". Please create new codes when needed:\\ncreativity: Captures instances where participants highlight the importance of an environment that encourages innovative thinking and the freedom to explore new ideas without fear of judgment or failure.\\npositive work environment: Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.\\n\\nBelow, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\nEXAMPLE INPUT:\\nThe training programs are really beneficial.\\n\\nEXAMPLE OUTPUT:\\nThe **training programs**<sup>professional development</sup> are really beneficial.\\n\\nEXAMPLE INPUT:\\nI feel valued and appreciated by my team.\\n\\nEXAMPLE OUTPUT:\\n**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\\n\\nACTUAL INPUT:\\nLearning and development programs are easily accessible and beneficial.\\n\\nMAIN INSIGHTS:\\nThe text contains no insights relevant to the research question.']\n",
      "max_tokens: 150\n",
      "use_cache: True\n",
      "temperature: 0\n",
      "system_message: None\n",
      "stop: None\n",
      "OK 1\n",
      "Checking for None values in API_type, model, and prompt_batch...\n",
      "API_type: OpenAI\n",
      "model: gpt-4o\n",
      "prompt_batch element at index 0: You are given a text to code inductively and the main insights from the text. Please carry out the following task:\n",
      "- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\n",
      "- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\n",
      "- Ignore text that is not insightful with regards to the research question.\n",
      "- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\n",
      "\n",
      "Some examples of codes in the format \"{code}: {description}\". Please create new codes when needed:\n",
      "creativity: Captures instances where participants highlight the importance of an environment that encourages innovative thinking and the freedom to explore new ideas without fear of judgment or failure.\n",
      "positive work environment: Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.\n",
      "\n",
      "Below, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "The training programs are really beneficial.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "The **training programs**<sup>professional development</sup> are really beneficial.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "I feel valued and appreciated by my team.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\n",
      "\n",
      "ACTUAL INPUT:\n",
      "Learning and development programs are easily accessible and beneficial.\n",
      "\n",
      "MAIN INSIGHTS:\n",
      "The text contains no insights relevant to the research question.\n",
      "OK 2\n",
      "OK 3\n",
      "continuations for loop: ['Learning and development programs are easily accessible and beneficial.']\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "\n",
    "# Sample inputs for function arguments\n",
    "\n",
    "texts = [\n",
    "    \"The companyâ€™s culture promotes teamwork and employee satisfaction.\",\n",
    "    \"Working here offers many opportunities for growth and learning.\",\n",
    "    \"I am getting more creative working in such an positive environment.\",\n",
    "    \"The management is supportive and values employee well-being.\",\n",
    "    \"Learning and development programs are easily accessible and beneficial.\",\n",
    "]\n",
    "\n",
    "research_question = \"What are the key elements of a positive work environment?\"\n",
    "\n",
    "few_shot_examples = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"I feel valued and appreciated by my team.\",\n",
    "        \"The training programs are really beneficial.\"\n",
    "    ],\n",
    "    \"coded_text\": [\n",
    "        \"**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\",\n",
    "        \"The **training programs**<sup>professional development</sup> are really beneficial.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "gpt_model = \"gpt-4o\"  # or another preferred GPT model\n",
    "use_cache = True\n",
    "max_tokens = 150  # Specify maximum tokens for each prompt if necessary\n",
    "verbose = True\n",
    "\n",
    "# Now you can run:\n",
    "coded_texts, code_descriptions = code_inductively_with_code_consistency(\n",
    "    texts=texts,\n",
    "    research_question=research_question,\n",
    "    few_shot_examples=few_shot_examples,\n",
    "    gpt_model=gpt_model,\n",
    "    use_cache=use_cache,\n",
    "    max_tokens=max_tokens,\n",
    "    verbose=verbose\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The companyâ€™s culture promotes **teamwork**<sup>positive work environment</sup> and **employee satisfaction**<sup>positive work environment</sup>.',\n",
       " 'Working here offers many opportunities for growth and learning.',\n",
       " 'I am getting more **creative**<sup>creativity</sup> working in such a **positive environment**<sup>positive work environment</sup>.',\n",
       " 'The **management is supportive and values employee well-being**<sup>positive work environment</sup>.',\n",
       " 'Learning and development programs are easily accessible and beneficial.']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'positive work environment': 'Captures instances where participants highlight the importance of collaborative efforts and mutual support among colleagues as a fundamental aspect of a positive work environment.',\n",
       " 'creativity': 'Captures instances where participants highlight the importance of an environment that encourages innovative thinking and the freedom to explore new ideas without fear of judgment or failure.'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 3\n",
      "prompt_batch for loop: [\"You are given a text to code inductively. Please carry out the following task:\\n- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\\n- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\\n- Ignore text that is not insightful with regards to the research question.\\n- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\nBelow, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\nEXAMPLE INPUT:\\nI feel recognized for my contributions to the team's success.\\n\\nEXAMPLE OUTPUT:\\nI feel **recognized for my contributions**<sup>recognition</sup> to the team's success.\\n\\nEXAMPLE INPUT:\\nI am motivated by the clear career paths available in this company.\\n\\nEXAMPLE OUTPUT:\\n**I am motivated by the clear career paths**<sup>career growth</sup> available in this company.\\n\\nACTUAL INPUT:\\nI feel valued by my colleagues and supported by my team leader.\", \"You are given a text to code inductively. Please carry out the following task:\\n- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\\n- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\\n- Ignore text that is not insightful with regards to the research question.\\n- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\nBelow, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\nEXAMPLE INPUT:\\nI am motivated by the clear career paths available in this company.\\n\\nEXAMPLE OUTPUT:\\n**I am motivated by the clear career paths**<sup>career growth</sup> available in this company.\\n\\nEXAMPLE INPUT:\\nI feel recognized for my contributions to the team's success.\\n\\nEXAMPLE OUTPUT:\\nI feel **recognized for my contributions**<sup>recognition</sup> to the team's success.\\n\\nACTUAL INPUT:\\nThere is a lot of opportunity for growth in my current role.\", \"You are given a text to code inductively. Please carry out the following task:\\n- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\\n- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\\n- Ignore text that is not insightful with regards to the research question.\\n- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\nBelow, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\nEXAMPLE INPUT:\\nI am motivated by the clear career paths available in this company.\\n\\nEXAMPLE OUTPUT:\\n**I am motivated by the clear career paths**<sup>career growth</sup> available in this company.\\n\\nEXAMPLE INPUT:\\nI feel recognized for my contributions to the team's success.\\n\\nEXAMPLE OUTPUT:\\nI feel **recognized for my contributions**<sup>recognition</sup> to the team's success.\\n\\nACTUAL INPUT:\\nThe office culture encourages collaboration and creativity.\"]\n",
      "Arguments:\n",
      "model: gpt-4o\n",
      "prompt_batch: [\"You are given a text to code inductively. Please carry out the following task:\\n- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\\n- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\\n- Ignore text that is not insightful with regards to the research question.\\n- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\nBelow, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\nEXAMPLE INPUT:\\nI feel recognized for my contributions to the team's success.\\n\\nEXAMPLE OUTPUT:\\nI feel **recognized for my contributions**<sup>recognition</sup> to the team's success.\\n\\nEXAMPLE INPUT:\\nI am motivated by the clear career paths available in this company.\\n\\nEXAMPLE OUTPUT:\\n**I am motivated by the clear career paths**<sup>career growth</sup> available in this company.\\n\\nACTUAL INPUT:\\nI feel valued by my colleagues and supported by my team leader.\", \"You are given a text to code inductively. Please carry out the following task:\\n- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\\n- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\\n- Ignore text that is not insightful with regards to the research question.\\n- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\nBelow, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\nEXAMPLE INPUT:\\nI am motivated by the clear career paths available in this company.\\n\\nEXAMPLE OUTPUT:\\n**I am motivated by the clear career paths**<sup>career growth</sup> available in this company.\\n\\nEXAMPLE INPUT:\\nI feel recognized for my contributions to the team's success.\\n\\nEXAMPLE OUTPUT:\\nI feel **recognized for my contributions**<sup>recognition</sup> to the team's success.\\n\\nACTUAL INPUT:\\nThere is a lot of opportunity for growth in my current role.\", \"You are given a text to code inductively. Please carry out the following task:\\n- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\\n- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\\n- Ignore text that is not insightful with regards to the research question.\\n- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\\n\\nBelow, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\\n\\nEXAMPLE INPUT:\\nI am motivated by the clear career paths available in this company.\\n\\nEXAMPLE OUTPUT:\\n**I am motivated by the clear career paths**<sup>career growth</sup> available in this company.\\n\\nEXAMPLE INPUT:\\nI feel recognized for my contributions to the team's success.\\n\\nEXAMPLE OUTPUT:\\nI feel **recognized for my contributions**<sup>recognition</sup> to the team's success.\\n\\nACTUAL INPUT:\\nThe office culture encourages collaboration and creativity.\"]\n",
      "max_tokens: 150\n",
      "use_cache: True\n",
      "temperature: 0\n",
      "system_message: None\n",
      "stop: None\n",
      "OK 1\n",
      "Checking for None values in API_type, model, and prompt_batch...\n",
      "API_type: OpenAI\n",
      "model: gpt-4o\n",
      "prompt_batch element at index 0: You are given a text to code inductively. Please carry out the following task:\n",
      "- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\n",
      "- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\n",
      "- Ignore text that is not insightful with regards to the research question.\n",
      "- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\n",
      "\n",
      "Below, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "I feel recognized for my contributions to the team's success.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "I feel **recognized for my contributions**<sup>recognition</sup> to the team's success.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "I am motivated by the clear career paths available in this company.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "**I am motivated by the clear career paths**<sup>career growth</sup> available in this company.\n",
      "\n",
      "ACTUAL INPUT:\n",
      "I feel valued by my colleagues and supported by my team leader.\n",
      "prompt_batch element at index 1: You are given a text to code inductively. Please carry out the following task:\n",
      "- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\n",
      "- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\n",
      "- Ignore text that is not insightful with regards to the research question.\n",
      "- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\n",
      "\n",
      "Below, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "I am motivated by the clear career paths available in this company.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "**I am motivated by the clear career paths**<sup>career growth</sup> available in this company.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "I feel recognized for my contributions to the team's success.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "I feel **recognized for my contributions**<sup>recognition</sup> to the team's success.\n",
      "\n",
      "ACTUAL INPUT:\n",
      "There is a lot of opportunity for growth in my current role.\n",
      "prompt_batch element at index 2: You are given a text to code inductively. Please carry out the following task:\n",
      "- Respond by repeating the original text, but highlighting the coded statements by surrounding the statements with double asterisks, as if they were bolded text in a Markdown document.\n",
      "- Include the associated code(s) immediately after the statement, separated by a semicolon and enclosed in <sup></sup> tags, as if they were superscript text in a Markdown document.\n",
      "- Ignore text that is not insightful with regards to the research question.\n",
      "- Preserve exact formatting of the original text. Do not correct typos or remove unnecessary spaces.\n",
      "\n",
      "Below, I first give you examples of the output you should produce given an example input. After that, I give you the actual input to process.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "I am motivated by the clear career paths available in this company.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "**I am motivated by the clear career paths**<sup>career growth</sup> available in this company.\n",
      "\n",
      "EXAMPLE INPUT:\n",
      "I feel recognized for my contributions to the team's success.\n",
      "\n",
      "EXAMPLE OUTPUT:\n",
      "I feel **recognized for my contributions**<sup>recognition</sup> to the team's success.\n",
      "\n",
      "ACTUAL INPUT:\n",
      "The office culture encourages collaboration and creativity.\n",
      "OK 2\n",
      "OK 3\n",
      "continuations for loop: ['I feel **valued by my colleagues**<sup>recognition</sup> and **supported by my team leader**<sup>support</sup>.', 'There is a lot of **opportunity for growth**<sup>career growth</sup> in my current role.', 'The office culture **encourages collaboration and creativity**<sup>work environment</sup>.']\n",
      "Coded Texts:\n",
      "I feel **valued by my colleagues**<sup>recognition</sup> and **supported by my team leader**<sup>support</sup>.\n",
      "There is a lot of **opportunity for growth**<sup>career growth</sup> in my current role.\n",
      "The office culture **encourages collaboration and creativity**<sup>work environment</sup>.\n"
     ]
    }
   ],
   "source": [
    "# Define sample inputs for the function\n",
    "\n",
    "texts = [\n",
    "    \"I feel valued by my colleagues and supported by my team leader.\",\n",
    "    \"There is a lot of opportunity for growth in my current role.\",\n",
    "    \"The office culture encourages collaboration and creativity.\"\n",
    "]\n",
    "\n",
    "research_question = \"What are the elements of a positive work environment?\"\n",
    "\n",
    "# Few-shot examples as a DataFrame, showing how coding should look\n",
    "import pandas as pd\n",
    "\n",
    "few_shot_examples = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"I am motivated by the clear career paths available in this company.\",\n",
    "        \"I feel recognized for my contributions to the team's success.\"\n",
    "    ],\n",
    "    \"coded_text\": [\n",
    "        \"**I am motivated by the clear career paths**<sup>career growth</sup> available in this company.\",\n",
    "        \"I feel **recognized for my contributions**<sup>recognition</sup> to the team's success.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "gpt_model = \"gpt-4o\"  # or another preferred GPT model\n",
    "use_cache = True\n",
    "max_tokens = 150  # Optional, can let the function determine this\n",
    "verbose = True\n",
    "\n",
    "# Call the function with the sample inputs\n",
    "coded_texts = code_inductively(\n",
    "    texts=texts,\n",
    "    research_question=research_question,\n",
    "    few_shot_examples=few_shot_examples,\n",
    "    gpt_model=gpt_model,\n",
    "    use_cache=use_cache,\n",
    "    max_tokens=max_tokens,\n",
    "    verbose=verbose\n",
    ")\n",
    "\n",
    "# Output the coded texts to check the results\n",
    "print(\"Coded Texts:\")\n",
    "for coded_text in coded_texts:\n",
    "    print(coded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I feel **valued by my colleagues**<sup>recognition</sup> and **supported by my team leader**<sup>support</sup>.',\n",
       " 'There is a lot of **opportunity for growth**<sup>career growth</sup> in my current role.',\n",
       " 'The office culture **encourages collaboration and creativity**<sup>work environment</sup>.']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "\n",
    "# Sample inputs for function arguments\n",
    "\n",
    "texts = [\n",
    "    \"The companyâ€™s culture promotes teamwork and employee satisfaction.\",\n",
    "    \"Working here offers many opportunities for growth and learning.\",\n",
    "    \"I am getting more creative working in such an positive environment.\",\n",
    "    \"The management is supportive and values employee well-being.\",\n",
    "    \"Learning and development programs are easily accessible and beneficial.\",\n",
    "]\n",
    "\n",
    "research_question = \"What are the key elements of a positive work environment?\"\n",
    "\n",
    "few_shot_examples = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"I feel valued and appreciated by my team.\",\n",
    "        \"The training programs are really beneficial.\"\n",
    "    ],\n",
    "    \"coded_text\": [\n",
    "        \"**I feel valued and appreciated**<sup>employee recognition</sup> by my team.\",\n",
    "        \"The **training programs**<sup>professional development</sup> are really beneficial.\"\n",
    "    ]\n",
    "})\n",
    "\n",
    "gpt_model = \"gpt-4o\"  # or another preferred GPT model\n",
    "use_cache = True\n",
    "max_tokens = 150  # Specify maximum tokens for each prompt if necessary\n",
    "verbose = True\n",
    "\n",
    "# Now you can run:\n",
    "coded_texts, code_descriptions = code_inductively_with_code_consistency(\n",
    "    texts=texts,\n",
    "    research_question=research_question,\n",
    "    few_shot_examples=few_shot_examples,\n",
    "    gpt_model=gpt_model,\n",
    "    use_cache=use_cache,\n",
    "    max_tokens=max_tokens,\n",
    "    verbose=verbose\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
