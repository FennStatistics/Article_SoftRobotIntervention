{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"meta-llama/Meta-Llama-3-70B-Instruct\" # meta-llama/Llama-3.1-8B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Assuming 'src' is one level down (in the current directory or a subdirectory)\n",
    "path_to_src = os.path.join('src')  # Moves one level down to 'src' folder\n",
    "\n",
    "# Add the path to sys.path\n",
    "sys.path.append(path_to_src)\n",
    "\n",
    "# Now you can import your API_key module\n",
    "import API_key as key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='\\n<Context>\\nYou are a knowledgeable assistant whose task is to provide two arrays: one for the names of the capitals and another for the primary languages spoken in the provided list of countries. \\nRespond solely with the information requested, formatted as specified.\\n</Context>\\n\\n<Data Structure>\\nThe provided countries are simply an array of country names.\\n</Data Structure>\\n\\n<Task>\\nWrite a JSON object containing two arrays: \"capital_names\" for the names of the requested capitals and \"languages\" for the primary languages spoken in those capitals. \\nEnsure that the information is structured as follows:\\n\\n{\\n  \"capitals\": [\\n   {\\n      \"name\": \"name1\",\\n      \"language\": \"language1\"\\n    },\\n    {\\n      \"name\": \"name2\",\\n      \"language\": \"language2\"\\n    },\\n    {\\n      \"name\": \"name3\",\\n      \"language\": \"language3\"\\n    }\\n  ]\\n}\\n\\nPlease respond with the entire JSON structure as a dictionary called \"capitals\", exactly as shown above, without any additional formatting or text.\\n</Task>\\n', additional_kwargs={}, response_metadata={}), HumanMessage(content='\\nWhat are the capitals of Germany, USA, France, China, Australia, and South Africa, along with the languages spoken there?\\n', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# Simplified system template\n",
    "system_template = \"\"\"\n",
    "<Context>\n",
    "You are a knowledgeable assistant whose task is to provide two arrays: one for the names of the capitals and another for the primary languages spoken in the provided list of countries. \n",
    "Respond solely with the information requested, formatted as specified.\n",
    "</Context>\n",
    "\n",
    "<Data Structure>\n",
    "The provided countries are simply an array of country names.\n",
    "</Data Structure>\n",
    "\n",
    "<Task>\n",
    "Write a JSON object containing two arrays: \"capital_names\" for the names of the requested capitals and \"languages\" for the primary languages spoken in those capitals. \n",
    "Ensure that the information is structured as follows:\n",
    "\n",
    "{{\n",
    "  \"capitals\": [\n",
    "   {{\n",
    "      \"name\": \"name1\",\n",
    "      \"language\": \"language1\"\n",
    "    }},\n",
    "    {{\n",
    "      \"name\": \"name2\",\n",
    "      \"language\": \"language2\"\n",
    "    }},\n",
    "    {{\n",
    "      \"name\": \"name3\",\n",
    "      \"language\": \"language3\"\n",
    "    }}\n",
    "  ]\n",
    "}}\n",
    "\n",
    "Please respond with the entire JSON structure as a dictionary called \"capitals\", exactly as shown above, without any additional formatting or text.\n",
    "</Task>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Simplified user template\n",
    "user_template = \"\"\"\n",
    "What are the capitals of {userinput}, along with the languages spoken there?\n",
    "\"\"\"\n",
    "\n",
    "# Set up the ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", user_template)]\n",
    ")\n",
    "\n",
    "# Test the invoke with simplified templates\n",
    "result = prompt_template.invoke({\"userinput\": \"Germany, USA, France, China, Australia, and South Africa\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "\n",
    "# Initialize the ChatOpenAI model\n",
    "model = ChatOpenAI(model=model_name, openai_api_key=key.hugging_api_key, openai_api_base=\"https://api-inference.huggingface.co/v1/\", max_tokens=500, temperature=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 405\n",
      "\tPrompt Tokens: 255\n",
      "\tCompletion Tokens: 150\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0\n",
      "Total Tokens: 405\n",
      "Prompt Tokens: 255\n",
      "Completion Tokens: 150\n",
      "Total Cost (USD): $0.0\n"
     ]
    }
   ],
   "source": [
    "# Configure the model with structured output\n",
    "#structured_llm = model.with_structured_output(json_schema, include_raw=True)\n",
    "#chain = prompt_template | structured_llm\n",
    "\n",
    "chain = prompt_template | model\n",
    "\n",
    "# Execute the model and output response details\n",
    "with get_openai_callback() as cb:\n",
    "    response = chain.invoke(\n",
    "        {\"userinput\": \"Germany, USA, France, China, Australia, Angola, Egypt\"}\n",
    "    )\n",
    "    print(cb)\n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"capitals\": [\n",
      "    {\n",
      "      \"name\": \"Berlin\",\n",
      "      \"language\": \"German\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Washington, D.C.\",\n",
      "      \"language\": \"English\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Paris\",\n",
      "      \"language\": \"French\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Beijing\",\n",
      "      \"language\": \"Mandarin Chinese\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Canberra\",\n",
      "      \"language\": \"English\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Luanda\",\n",
      "      \"language\": \"Portuguese\"\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"Cairo\",\n",
      "      \"language\": \"Arabic\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "               name          language\n",
      "0            Berlin            German\n",
      "1  Washington, D.C.           English\n",
      "2             Paris            French\n",
      "3           Beijing  Mandarin Chinese\n",
      "4          Canberra           English\n",
      "5            Luanda        Portuguese\n",
      "6             Cairo            Arabic\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "print(response.content)\n",
    "\n",
    "\n",
    "# Load the JSON data\n",
    "data = json.loads(response.content)\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data['capitals'])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = Chroma(persist_directory=CHROMA_PATH, embedding_function=OpenAIEmbeddings(api_key=openAI_key))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "from langchain.vectorstores import utils as chromautils\n",
    "\n",
    "# ChromaDB doesn't support complex metadata, e.g. lists, so we drop it here.\n",
    "# If you're using a different vector store, you may not need to do this\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"BAAI/bge-base-en-v1.5\")\n",
    "#vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "#retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "\n",
    "vectorstore_RR = Chroma.from_documents(documents=splits_RR, embedding=embeddings)\n",
    "retriever_RR = vectorstore_RR.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules:\n",
    "import pandas as pd\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import math\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# to read pdf files\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# to read text of pdf files\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "\n",
    "# get external variables or functions:\n",
    "import src.API_key as key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data to feed ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\DATEN\\PHD\\Article_SoftRobotIntervention\\Analyses\\main study - ChatGPT\n",
      "['.venv', 'data', 'output', 'runChatGPT.ipynb', 'runChatGPT_new.ipynb', 'src', 'testChatGPT.ipynb', 'v01', 'v02']\n"
     ]
    }
   ],
   "source": [
    "## set working environment\n",
    "#> Get the current working directory\n",
    "print(os.getcwd())\n",
    "directory = os.getcwd()\n",
    "\n",
    "# List files in the current working directory\n",
    "files = os.listdir('.')\n",
    "# Display the list of files\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Scenario Texts\n",
    "This should be the final scenario texts in English of the two robots:\n",
    "\n",
    "* rescue robot\n",
    "* socially assistive robot\n",
    "\n",
    "\n",
    "source: https://python.langchain.com/v0.2/docs/tutorials/pdf_qa/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length PDF rescue robot: 6\n",
      "length PDF socially assistive robot: 6\n"
     ]
    }
   ],
   "source": [
    "# Load the scenario texts of the rescue robot\n",
    "file_path = directory + \"/data/scenario texts/\" + \"rescue robot\" + \".pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "doc_RR = loader.load()\n",
    "print(\"length PDF rescue robot:\", len(doc_RR))\n",
    "\n",
    "# Load the scenario texts of the socially assistive robot\n",
    "file_path = directory + \"/data/scenario texts/\" + \"socially assistive robot\" + \".pdf\"\n",
    "loader = PyPDFLoader(file_path)\n",
    "doc_SAR = loader.load()\n",
    "print(\"length PDF socially assistive robot:\", len(doc_SAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Using a text splitter, the loaded documents will be split into smaller documents that can more easily fit into an LLM's context window, then load them into a vector store. Then a retriever from the vector store is created for use in our RAG chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=800, chunk_overlap=200)\n",
    "splits_RR = text_splitter.split_documents(doc_RR)\n",
    "vectorstore_RR = Chroma.from_documents(documents=splits_RR, embedding=OpenAIEmbeddings(openai_api_key=key.openai_api_key))\n",
    "\n",
    "retriever_RR = vectorstore_RR.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.chroma.Chroma at 0x1ea4bba4550>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorstore_RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\", openai_api_key=key.openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What are the advantages of soft robots?',\n",
       " 'context': [Document(metadata={'page': 4, 'source': 'c:\\\\DATEN\\\\PHD\\\\Article_SoftRobotIntervention\\\\Analyses\\\\main study - ChatGPT/data/scenario texts/rescue robot.pdf'}, page_content='––––––––––––  Second  Page in Experiment (Intervention) ––––––––––––   \\nSoft Robots for Search and Rescue Missions  \\nBenefits of soft robots for search and rescue missions might be:  \\n●  Access to areas unreachable or too dangerous for human rescuers  \\n●  Delivery  of essential supplies (water, food, medicine) until victims are safely  \\nextracted  \\n●  Reduced risk of injury to victims due to their flexibility and adaptability  \\nPossible risks of soft robots for search and rescue missions might be:  \\n●  Algorithms  guiding soft robots may be biased, leading to unfair or \\ndiscriminatory outcomes, regarding (i) where to concentrate rescue efforts, (ii) \\nwhom to search for first, (iii) who should be given priority treatment, (iv) who \\nmust be left to wait'),\n",
       "  Document(metadata={'page': 4, 'source': 'c:\\\\DATEN\\\\PHD\\\\Article_SoftRobotIntervention\\\\Analyses\\\\main study - ChatGPT/data/scenario texts/rescue robot.pdf'}, page_content='––––––––––––  Second  Page in Experiment (Intervention) ––––––––––––   \\nSoft Robots for Search and Rescue Missions  \\nBenefits of soft robots for search and rescue missions might be:  \\n●  Access to areas unreachable or too dangerous for human rescuers  \\n●  Delivery  of essential supplies (water, food, medicine) until victims are safely  \\nextracted  \\n●  Reduced risk of injury to victims due to their flexibility and adaptability  \\nPossible risks of soft robots for search and rescue missions might be:  \\n●  Algorithms  guiding soft robots may be biased, leading to unfair or \\ndiscriminatory outcomes, regarding (i) where to concentrate rescue efforts, (ii) \\nwhom to search for first, (iii) who should be given priority treatment, (iv) who \\nmust be left to wait'),\n",
       "  Document(metadata={'page': 4, 'source': 'c:\\\\DATEN\\\\PHD\\\\Article_SoftRobotIntervention\\\\Analyses\\\\main study - ChatGPT/data/scenario texts/rescue robot.pdf'}, page_content='––––––––––––  Second  Page in Experiment (Intervention) ––––––––––––   \\nSoft Robots for Search and Rescue Missions  \\nBenefits of soft robots for search and rescue missions might be:  \\n●  Access to areas unreachable or too dangerous for human rescuers  \\n●  Delivery  of essential supplies (water, food, medicine) until victims are safely  \\nextracted  \\n●  Reduced risk of injury to victims due to their flexibility and adaptability  \\nPossible risks of soft robots for search and rescue missions might be:  \\n●  Algorithms  guiding soft robots may be biased, leading to unfair or \\ndiscriminatory outcomes, regarding (i) where to concentrate rescue efforts, (ii) \\nwhom to search for first, (iii) who should be given priority treatment, (iv) who \\nmust be left to wait'),\n",
       "  Document(metadata={'page': 3, 'source': 'c:\\\\DATEN\\\\PHD\\\\Article_SoftRobotIntervention\\\\Analyses\\\\main study - ChatGPT/data/scenario texts/rescue robot.pdf'}, page_content='–––––––––––––  First Page in Exp eriment (Intervention) ––––––––––––   \\nPlease read the following information on soft robots carefully. \\nAfterwards we will ask you to adjust your CAM.  \\nCurrently, there is a trend towards using a new type of so -called soft robots for \\nsearch and rescue missions. Soft robots are a new kind of robot which are designed \\nto mimic the properties of living entities such as animals. Unlike normal robots, which \\nare typically composed of hard materials like metal or hard -plastic, soft robots do not \\nhave electronic devices in them and are made of flexible, soft materials like silicone. \\nThey often have natural shapes and can bend, twist, and stretch like living \\norganis ms, such as snakes or octopi. Designed with inspiration from living entities, \\nthese soft robots often look and feel more lifelike than rigid robots.'),\n",
       "  Document(metadata={'page': 3, 'source': 'c:\\\\DATEN\\\\PHD\\\\Article_SoftRobotIntervention\\\\Analyses\\\\main study - ChatGPT/data/scenario texts/rescue robot.pdf'}, page_content='–––––––––––––  First Page in Exp eriment (Intervention) ––––––––––––   \\nPlease read the following information on soft robots carefully. \\nAfterwards we will ask you to adjust your CAM.  \\nCurrently, there is a trend towards using a new type of so -called soft robots for \\nsearch and rescue missions. Soft robots are a new kind of robot which are designed \\nto mimic the properties of living entities such as animals. Unlike normal robots, which \\nare typically composed of hard materials like metal or hard -plastic, soft robots do not \\nhave electronic devices in them and are made of flexible, soft materials like silicone. \\nThey often have natural shapes and can bend, twist, and stretch like living \\norganis ms, such as snakes or octopi. Designed with inspiration from living entities, \\nthese soft robots often look and feel more lifelike than rigid robots.')],\n",
       " 'answer': 'Soft robots have advantages such as accessing areas unreachable or too dangerous for human rescuers, delivering essential supplies like water, food, and medicine, and reducing the risk of injury to victims due to their flexibility and adaptability.'}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt = (\n",
    "    \"You are an assistant for question-answering tasks. \"\n",
    "    \"Use the following pieces of retrieved context to answer \"\n",
    "    \"the question. If you don't know the answer, say that you \"\n",
    "    \"don't know. Use three sentences maximum and keep the \"\n",
    "    \"answer concise.\"\n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, prompt)\n",
    "rag_chain = create_retrieval_chain(retriever_RR, question_answer_chain)\n",
    "\n",
    "results = rag_chain.invoke({\"input\": \"What are the advantages of soft robots?\"})\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "essential supplies, and autonomously assisting in the rescue of victims, these robots \n",
      "can enhance the efficiency and effectiveness of rescue operations.  \n",
      "While robots f or search and rescue are still in the development phase, it is important \n",
      "to consider the ethical aspects (risks and benefits) of these technologies.\n",
      "{'page': 0, 'source': 'c:\\\\DATEN\\\\PHD\\\\Article_SoftRobotIntervention\\\\Analyses\\\\main study - ChatGPT/data/scenario texts/rescue robot.pdf'}\n"
     ]
    }
   ],
   "source": [
    "print(results[\"context\"][0].page_content)\n",
    "print(results[\"context\"][0].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ERROR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load .xlsx files (lists of words)\n",
    "This should be the final scenario texts in English of the two robots combined and seperated:\n",
    "\n",
    "* rescue robot_multipleSheets\n",
    "* socially assistive robot_multipleSheets\n",
    "* rescue robot_socially assistive robot_multipleSheets\n",
    "\n",
    "sources:\n",
    "* https://python.langchain.com/v0.2/docs/tutorials/llm_chain/ (Build a Simple LLM Application with LCEL)\n",
    "* https://python.langchain.com/v0.2/docs/how_to/structured_output/ (How to return structured data from a model)\n",
    "* https://python.langchain.com/v0.2/docs/how_to/llm_token_usage_tracking/ (How to track token usage for LLMs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the xlsx file of the rescue robot and the socially assistive robot combined\n",
    "# Path to your Excel file\n",
    "file_path = directory + \"/data/\" + \"rescue robot_socially assistive robot_multipleSheets\" + \".xlsx\"\n",
    "# Load the Excel file\n",
    "excel_data = pd.ExcelFile(file_path)\n",
    "# Print the sheet names\n",
    "print(\"Sheet names combined:\", excel_data.sheet_names)\n",
    "# Load all sheets into a dictionary of dataframes\n",
    "all_sheets_Combined = {sheet_name: excel_data.parse(sheet_name) for sheet_name in excel_data.sheet_names}\n",
    "\n",
    "\n",
    "## Load the xlsx file of the rescue robot and the socially assistive robot combined\n",
    "# Path to your Excel file\n",
    "file_path = directory + \"/data/\" + \"rescue robot_multipleSheets\" + \".xlsx\"\n",
    "# Load the Excel file\n",
    "excel_data = pd.ExcelFile(file_path)\n",
    "# Print the sheet names\n",
    "print(\"Sheet names RR:\", excel_data.sheet_names)\n",
    "# Load all sheets into a dictionary of dataframes\n",
    "all_sheets_RR = {sheet_name: excel_data.parse(sheet_name) for sheet_name in excel_data.sheet_names}\n",
    "\n",
    "\n",
    "\n",
    "## Load the xlsx file of the rescue robot and the socially assistive robot combined\n",
    "# Path to your Excel file\n",
    "file_path = directory + \"/data/\" + \"socially assistive robot_multipleSheets\" + \".xlsx\"\n",
    "# Load the Excel file\n",
    "excel_data = pd.ExcelFile(file_path)\n",
    "# Print the sheet names\n",
    "print(\"Sheet names SAR:\", excel_data.sheet_names)\n",
    "# Load all sheets into a dictionary of dataframes\n",
    "all_sheets_SAR = {sheet_name: excel_data.parse(sheet_name) for sheet_name in excel_data.sheet_names}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviations_dict = {\n",
    "    'RCPP': 'perceived positive usefulness (rest category, refers to a classification of arguments that do not fit into any of the predefined categories)',\n",
    "    'LC': 'perceived low costs',\n",
    "    'T': 'perceived trust',\n",
    "    'SIP': 'perceived positive social impact',\n",
    "    'HRIP': 'perceived positive Human-Robot-Interaction',\n",
    "    'AN': 'perceived negative anthropomorphism',\n",
    "    'SIN': 'perceived positive social impact',\n",
    "    'R': 'perceived risks',\n",
    "    'HC': 'perceived high costs',\n",
    "    'RCN': 'neutral rest category (rest category refers to a classification of arguments that do not fit into any of the predefined categories)',\n",
    "    'SA': 'perceived safety',\n",
    "    'TP': 'perceived technological possibilities',\n",
    "    'TL': 'perceived technological limitations',\n",
    "    'RCPN': 'perceived negative usefulness (rest category, refers to a classification of arguments that do not fit into any of the predefined categories)',\n",
    "    'HRIN': 'perceived negative Human-Robot-Interaction',\n",
    "    'MT': 'perceived mistrust',\n",
    "    'RCA': 'ambivalent rest category (rest category refers to a classification of arguments that do not fit into any of the predefined categories)',\n",
    "    'AP': 'perceived positive anthropomorphism'\n",
    "}\n",
    "\n",
    "print(abbreviations_dict)\n",
    "print(abbreviations_dict.keys())\n",
    "print(abbreviations_dict['AN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Access data from a specific sheet\n",
    "sheet_name = 'T'\n",
    "print(all_sheets_Combined[sheet_name].shape)\n",
    "print(all_sheets_RR[sheet_name].shape)\n",
    "print(all_sheets_SAR[sheet_name].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use dictionaries to map each word to its comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_multivalue_dict(df, key_col, value_col):\n",
    "    \"\"\"\n",
    "    Create a dictionary from a DataFrame where each key maps to a list of values.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    key_col (str): The column name to be used as keys.\n",
    "    value_col (str): The column name to be used as values.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A dictionary where each key maps to a list of values.\n",
    "    \"\"\"\n",
    "    # Remove rows with NaN in the key columns\n",
    "    df = df.dropna(subset=[key_col])\n",
    "\n",
    "    # Create a dictionary to map items to their comments, allowing for multiple comments per key\n",
    "    multivalue_dict = {}\n",
    "    for key, value in zip(df[key_col], df[value_col]):\n",
    "        if key in multivalue_dict:\n",
    "            multivalue_dict[key].append(value)\n",
    "        else:\n",
    "            multivalue_dict[key] = [value]\n",
    "\n",
    "    return multivalue_dict\n",
    "\n",
    "# Example usage\n",
    "data = {\n",
    "    'constant': ['a', 'b', 'a', 'c', 'b', np.nan],\n",
    "    'constant_comments': ['comment1', 'comment2', 'comment3', 'comment4', 'comment5', 'comment6']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_mapping = create_multivalue_dict(df, 'constant', 'constant_comments')\n",
    "print(df_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_dicts(dict1, dict2):\n",
    "    \"\"\"\n",
    "    Combine two dictionaries where each key maps to a list of values.\n",
    "    \n",
    "    Parameters:\n",
    "    dict1 (dict): The first dictionary.\n",
    "    dict2 (dict): The second dictionary.\n",
    "    \n",
    "    Returns:\n",
    "    dict: A combined dictionary where each key maps to a concatenated list of values.\n",
    "    \"\"\"\n",
    "    combined_dict = dict1.copy()\n",
    "    for key, values in dict2.items():\n",
    "        if key in combined_dict:\n",
    "            combined_dict[key].extend(values)\n",
    "        else:\n",
    "            combined_dict[key] = values\n",
    "    return combined_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with NaN in the key columns as they cannot be used as dictionary keys\n",
    "#> not sensitive to multiple identical keys: dict(zip(df['constant'], df['constant_comments']))\n",
    "df = all_sheets_Combined[sheet_name]\n",
    "\n",
    "constant_comments_mapping = create_multivalue_dict(df, 'constant', 'constant_comments')\n",
    "print(\"mapping constant x comments:\", constant_comments_mapping)\n",
    "print(len(constant_comments_mapping))\n",
    "\n",
    "new_comments_mapping = create_multivalue_dict(df, 'new', 'new_comments')\n",
    "print(\"mapping new x comments:\", new_comments_mapping)\n",
    "print(len(new_comments_mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide prompt template\n",
    "\n",
    "### for task to get main findings, differences and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"<Context>You are a researcher tasked with summarizing two wordlists that highlight people's assessments of rigid robots compared to soft robots. Laypersons were informed about the potential risks and benefits of {robots} through scenario texts. Initially, they listed their perceived risks and benefits of rigid robots in a list titled \"rigid.\" Subsequently, they learned about the trend towards soft robots, which are made of flexible, soft materials and are electronic-free. They then created a list titled \"soft\" to highlight the differences between rigid and soft robots. The overarching topic of the two lists is the {topicCategory}.</Context>\n",
    "\n",
    "<Data Structure>The lists \"rigid\" and \"soft\" are dictionaries where the keys are written arguments, and their corresponding values are one or more comments related to those arguments. The value [nan] indicates that no specific comment was provided for the respective entry. If there are multiple comments or missing entries ([nan]), it signifies that the respective argument was mentioned as many times as there are entries.</Data Structure>\n",
    "\n",
    "<Task>Write two concise bullet points: one highlighting the main findings of the provided \"rigid\" and \"soft\" lists combined, and the other detailing the differences between the provided \"rigid\" and \"soft\" lists. Each set of bullet points should contain a maximum of five items, focusing on the overarching argument structures. Additionally, provide a summary paragraph of no more than four sentences that encapsulates the main findings and the found differences. Do not use the term list, instead refer to the {robots}. Be scientific and neutral in your wording. Consider all provided information carefully. Check if you have provided the two lists of bullet points (called mainFindings and differences), and the summary paragraph (called summary).</Task>\"\"\"\n",
    "\n",
    "# !!!\n",
    "system_template = \"\"\"\n",
    "<Context>You are a researcher tasked with summarizing two wordlists that highlight people's assessments of rigid robots compared to soft robots. Laypersons were informed about the potential risks and benefits of {robots} through scenario texts. Initially, they listed their perceived risks and benefits of rigid robots in a list titled \"rigid.\" Subsequently, they learned about the trend towards soft robots, which are made of flexible, soft materials and are electronic-free. They then created a list titled \"soft\" to highlight the differences between rigid and soft robots. The overarching topic of the two lists is the {topicCategory}, whereby the topic involved {topicCategoryDetails}.</Context>\n",
    "\n",
    "<Data Structure>The lists \"rigid\" and \"soft\" are dictionaries where the keys are written arguments, and their corresponding values are one or more comments related to those arguments. The value [nan] indicates that no specific comment was provided for the respective entry. If there are multiple comments or missing entries ([nan]), it signifies that the respective argument was mentioned as many times as there are entries.</Data Structure>\n",
    "\n",
    "<Task>Write two concise bullet points: one highlighting the main findings of the provided \"rigid\" list, and the highlighting the main findings of the provided \"soft\" list. Each set of bullet points should contain a maximum of five items, focusing on the overarching argument structures. Additionally, provide a summary paragraph of no more than four sentences that encapsulates the main findings of both lists. Do not use the term list, instead refer to the {robots}. Be scientific and neutral in your wording. Consider all provided information carefully. Check if you have provided the two lists of bullet points (called mainFindingsRigid and mainFindingsSoft), and the summary paragraph (called summary).</Task>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "user_template = \"\"\"List \"rigid\": \n",
    "{rigid}\n",
    "\n",
    "List \"soft\": \n",
    "{soft}\"\"\"\n",
    "\n",
    "# rescue robots and socially assistive robots\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", user_template)]\n",
    ")\n",
    "\n",
    "result = prompt_template.invoke({\"robots\": \"rescue robots and socially assistive robots\", \"topicCategory\": abbreviations_dict[sheet_name], \"rigid\": constant_comments_mapping, \"soft\": new_comments_mapping})\n",
    "print(result)\n",
    "\n",
    "print(\"result:\", result)\n",
    "print(\"result.to_messages():\", result.to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for task to get superordinate categories within single categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"\"\"\n",
    "<Context>\n",
    "You are a researcher tasked with summarizing a list of words into generic/superordinate categories. Based on these categories, create a dictionary that assigns the respective subordinate terms (keys from the provided \"overallList\") to the generic terms. Laypersons were informed about the potential risks and benefits of rigid and soft {robots} through scenario texts. They then listed their perceived risks and benefits of rigid and soft robots in the \"overallList\" wordlist. The overarching topic of the list is {topicCategory}.\n",
    "</Context>\n",
    "\n",
    "<Data Structure>\n",
    "The list \"overallList\" is a dictionary where the keys are written arguments, and the corresponding values are one or more comments related to those arguments. The value [nan] indicates that no specific comment was provided for the respective entry. If there are multiple comments or missing entries ([nan]), it signifies that the respective argument was mentioned multiple times, emphasizing its importance.\n",
    "</Data Structure>\n",
    "\n",
    "<Task>\n",
    "Your task is to create two outputs:\n",
    "1. A list called \"listGeneric\" that contains the generic/superordinate categories. You may use no more than six different categories.\n",
    "2. A dictionary called \"dictionary\" that contains:\n",
    "   - Keys: The generic/superordinate categories.\n",
    "   - Values: The corresponding words (keys) from the \"overallList\" that have been summarized under each category.\n",
    "The dictionary must contain all corresponding words (keys) from the \"overallList\". If it is not possible to assign a specific word, please place it in a category called \"rest category\".\n",
    "</Task>\n",
    "\"\"\"\n",
    "\n",
    "# !!!\n",
    "system_template = \"\"\"\n",
    "<Context> You are a researcher tasked with summarizing a list of words into generic/superordinate categories. Based on these categories, create a dictionary that assigns the respective subordinate terms (keys from the provided \"overallList\") to the generic terms. Laypersons were informed about the potential risks and benefits of rigid and soft {robots} through scenario texts. They then listed their perceived risks and benefits of rigid and soft robots in the \"overallList\" wordlist. The overarching topic of the list is {topicCategory}, whereby the topic involved {topicCategoryDetails}.</Context>\n",
    "\n",
    "<Data Structure> The list \"overallList\" is a dictionary where the keys are written arguments, and the corresponding values are one or more comments related to those arguments. The value [nan] indicates that no specific comment was provided for the respective entry. If there are multiple comments or missing entries ([nan]), it signifies that the respective argument was mentioned multiple times, emphasizing its importance. </Data Structure>\n",
    "\n",
    "<Task> Your task is to create two outputs:\n",
    "1. A list called \"listGeneric\" that contains the generic/superordinate categories. You may use no more than six different categories.\n",
    "2. A dictionary called \"dictionary\" that contains: Keys (the generic/superordinate categories) and values (the corresponding words - keys - from the \"overallList\" that have been summarized under each category).\n",
    "The dictionary must contain all corresponding words (keys) from the \"overallList\". If it is not possible to assign a specific word, please place it in a category called \"rest category\".</Task>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "user_template = \"\"\"\n",
    "List \"overallList\": \n",
    "{overallList}\n",
    "\"\"\"\n",
    "\n",
    "# rescue robots and socially assistive robots\n",
    "prompt_template_SC = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", system_template), (\"user\", user_template)]\n",
    ")\n",
    "\n",
    "constant_new_comments_mapping = combine_dicts(constant_comments_mapping, new_comments_mapping)\n",
    "\n",
    "result = prompt_template_SC.invoke({\"robots\": \"rescue robots and socially assistive robots\", \"topicCategory\": abbreviations_dict[sheet_name], \"overallList\": constant_new_comments_mapping})\n",
    "print(result)\n",
    "\n",
    "print(\"result:\", result)\n",
    "print(\"result.to_messages():\", result.to_messages())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provide schemas for structured outputs\n",
    "\n",
    "### for task to get main findings, differences and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = {\n",
    "    \"title\": \"Outputs\",\n",
    "    \"description\": \"Bullet lists detailing the similarities and differences between the rigid and soft lists and a summary paragraph.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"mainFindings\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Bullet lists highlighting the main findings of the provided rigid and soft lists\",\n",
    "        },\n",
    "        \"differences\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Bullet lists detailing the differences between the rigid and soft lists\",\n",
    "        },\n",
    "          \"summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Summary paragraph that provides a summary of the main findings and the found differences\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"similarities\", \"differences\", \"summary\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for task to get superordinate categories within single categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema_notUsed = {\n",
    "    \"title\": \"Outputs\",\n",
    "    \"description\": \"List that contains the generic / superordinate categories and a dictionary, which assigns the respective subordinate terms to the generic terms.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"listGeneric\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"List that contains the generic / superordinate categories\",\n",
    "        },\n",
    "        \"dictionary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Dictionary that contains the keys, the generic / superordinate categories and the corresponding words that have been summarised under the respective category\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"listGeneric\", \"dictionary\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema_SC = {\n",
    "    \"title\": \"Outputs\",\n",
    "    \"description\": \"List that contains the generic/superordinate categories and a dictionary that assigns the respective subordinate terms to the generic terms.\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"listGeneric\": {\n",
    "            \"type\": \"array\",\n",
    "            \"description\": \"List that contains the generic/superordinate categories.\",\n",
    "            \"items\": {\n",
    "                \"type\": \"string\"\n",
    "            }\n",
    "        },\n",
    "        \"dictionary\": {\n",
    "            \"type\": \"object\",\n",
    "            \"description\": \"Dictionary that contains the generic/superordinate categories as keys and the corresponding words from the 'overallList' as values.\",\n",
    "            \"additionalProperties\": {\n",
    "                \"type\": \"array\",\n",
    "                \"items\": {\n",
    "                    \"type\": \"string\"\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"listGeneric\", \"dictionary\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define basic API call\n",
    "\n",
    "### for task to get main findings, differences and summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_API_call(\n",
    "    prompt,\n",
    "    robots,\n",
    "    topicCategory,\n",
    "    openai_api_key,\n",
    "    dictonaryRigid,\n",
    "    dictonarySoft,\n",
    "    json_schema,\n",
    "    model_name=\"gpt-4o\",\n",
    "    max_tokens=1000,\n",
    "):\n",
    "\n",
    "    # prompt = PromptTemplate(template=template)\n",
    "    seed = 123\n",
    "\n",
    "    model = ChatOpenAI(model=model_name, openai_api_key=openai_api_key, max_tokens=max_tokens, model_kwargs={\"seed\": seed}, temperature=0.0)\n",
    "       \n",
    "    structured_llm = model.with_structured_output(json_schema, include_raw=True)\n",
    "    chain = prompt | structured_llm\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        response = chain.invoke(\n",
    "            {\"robots\": robots, \"topicCategory\": topicCategory, \"rigid\": dictonaryRigid, \"soft\": dictonarySoft}\n",
    "        )\n",
    "        print(cb)\n",
    "    \n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for task to get superordinate categories within single categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_API_call_SC(\n",
    "    prompt,\n",
    "    robots,\n",
    "    topicCategory,\n",
    "    openai_api_key,\n",
    "    dictonaryCombined,\n",
    "    json_schema,\n",
    "    model_name=\"gpt-4o\",\n",
    "    max_tokens=1000,\n",
    "):\n",
    "\n",
    "    # prompt = PromptTemplate(template=template)\n",
    "    seed = 123\n",
    "\n",
    "    model = ChatOpenAI(model=model_name, openai_api_key=openai_api_key, max_tokens=max_tokens, model_kwargs={\"seed\": seed}, temperature=0.0)\n",
    "       \n",
    "    structured_llm = model.with_structured_output(json_schema, include_raw=True)\n",
    "    chain = prompt | structured_llm\n",
    "\n",
    "    with get_openai_callback() as cb:\n",
    "        response = chain.invoke(\n",
    "            {\"robots\": robots, \"topicCategory\": topicCategory, \"overallList\": dictonaryCombined}\n",
    "        )\n",
    "        print(cb)\n",
    "    \n",
    "    print(f\"Total Tokens: {cb.total_tokens}\")\n",
    "    print(f\"Prompt Tokens: {cb.prompt_tokens}\")\n",
    "    print(f\"Completion Tokens: {cb.completion_tokens}\")\n",
    "    print(f\"Total Cost (USD): ${cb.total_cost}\")\n",
    "        \n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run ChatGPT\n",
    "\n",
    "## Example (overall)\n",
    "\n",
    "only for main findings, difference, summary\n",
    "\n",
    "> Remark: The argument structures between the two types of robots differ significantly. Therefore, the robots are qualitatively summarized separately to ensure a clear and accurate comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"sheet_name: {sheet_name}\")\n",
    "print(f\"abbreviations_dict[sheet_name]: {abbreviations_dict[sheet_name]}\")\n",
    "\n",
    "\n",
    "result = basic_API_call(prompt=prompt_template,\n",
    "    robots=\"rescue robots and socially assistive robots\",\n",
    "    topicCategory=abbreviations_dict[sheet_name],\n",
    "    openai_api_key=key.openai_api_key,\n",
    "    dictonaryRigid=constant_comments_mapping, # overall\n",
    "    dictonarySoft=new_comments_mapping,\n",
    "    json_schema=json_schema,\n",
    "    model_name=\"gpt-4o\",\n",
    "    max_tokens=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 'parsed' section from the JSON data\n",
    "parsed_section = result.get('parsed', {})\n",
    "#print(parsed_section)\n",
    "# Extract the translations\n",
    "mainFindings = parsed_section.get('mainFindings')\n",
    "differences = parsed_section.get('differences')\n",
    "summary = parsed_section.get('summary')\n",
    "\n",
    "print(f\"result (raw): {result}\")\n",
    "print(f\"mainFindings: {mainFindings}\")\n",
    "print(f\"differences: {differences}\")\n",
    "print(f\"summary: {summary}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separately for robots (rescue robot and socially assistive robot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for rescue robots (main findings, difference, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "mainFindings = []\n",
    "differences = []\n",
    "summary = []\n",
    "rawResults = []\n",
    "\n",
    "\n",
    "for category in abbreviations_dict.keys():\n",
    "    print(f\"category: {category}\")\n",
    "    \n",
    "    # do not process the rest categories\n",
    "    if category not in ['RCPP', 'RCPN', 'RCA', 'RCN']:\n",
    "        df = all_sheets_RR[category]\n",
    "        constant_comments_mapping = create_multivalue_dict(df, 'constant', 'constant_comments')\n",
    "        # print(\"mapping constant x comments:\", constant_comments_mapping)\n",
    "        # print(len(constant_comments_mapping))\n",
    "        \n",
    "        new_comments_mapping = create_multivalue_dict(df, 'new', 'new_comments')\n",
    "        # print(\"mapping new x comments:\", new_comments_mapping)\n",
    "        # print(len(new_comments_mapping))\n",
    "    \n",
    "        result = basic_API_call(prompt=prompt_template,\n",
    "            robots=\"rescue robots\",\n",
    "            topicCategory=abbreviations_dict[category],\n",
    "            openai_api_key=key.openai_api_key,\n",
    "            dictonaryRigid=constant_comments_mapping,\n",
    "            dictonarySoft=new_comments_mapping,\n",
    "            json_schema=json_schema,\n",
    "            model_name=\"gpt-4o\",\n",
    "            max_tokens=1600, # increase limit\n",
    "        )\n",
    "        \n",
    "        # append raw results\n",
    "        categories.append(category)\n",
    "        rawResults.append(result)\n",
    "        \n",
    "        # append parsed results\n",
    "        parsed_section = result.get('parsed', {})\n",
    "        mainFindings.append(parsed_section.get('mainFindings'))\n",
    "        differences.append(parsed_section.get('differences'))\n",
    "        summary.append(parsed_section.get('summary'))\n",
    "        #print(\"length of mainFindings:\", len(parsed_section.get('mainFindings')))\n",
    "        #print(\"length of differences:\", len(parsed_section.get('differences')))\n",
    "        #print(\"length of summary:\", len(parsed_section.get('summary')))\n",
    "        \n",
    "# save file\n",
    "df_RR = pd.DataFrame({\n",
    "    'Category': categories,\n",
    "    'mainFindings': mainFindings,\n",
    "    'differences': differences,\n",
    "    'summary': summary,\n",
    "    'rawResults' : rawResults\n",
    "})\n",
    "\n",
    "# Path to your Excel file\n",
    "file_path = directory + \"/output/\" + \"rescue robot_ChatGPT\" + \".xlsx\"\n",
    "# save the dataframe to an Excel file\n",
    "df_RR.to_excel(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for rescue robots (get superordinate categories within single categories: listGeneric, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "mainFindings = []\n",
    "differences = []\n",
    "summary = []\n",
    "rawResults = []\n",
    "\n",
    "\n",
    "for category in abbreviations_dict.keys():\n",
    "    print(f\"category: {category}\")\n",
    "    \n",
    "    # do not process the rest categories\n",
    "    # not in ['RCPP', 'RCPN', 'RCA', 'RCN']:\n",
    "    if category in ['MT']:\n",
    "        df = all_sheets_RR[category]\n",
    "        constant_comments_mapping = create_multivalue_dict(df, 'constant', 'constant_comments')\n",
    "        # print(\"mapping constant x comments:\", constant_comments_mapping)\n",
    "        # print(len(constant_comments_mapping))\n",
    "        \n",
    "        new_comments_mapping = create_multivalue_dict(df, 'new', 'new_comments')\n",
    "        # print(\"mapping new x comments:\", new_comments_mapping)\n",
    "        # print(len(new_comments_mapping))\n",
    "        constant_new_comments_mapping = combine_dicts(constant_comments_mapping, new_comments_mapping)\n",
    "\n",
    "\n",
    "        result = basic_API_call_SC(prompt=prompt_template_SC,\n",
    "            robots=\"rescue robots\",\n",
    "            topicCategory=abbreviations_dict[category],\n",
    "            openai_api_key=key.openai_api_key,\n",
    "            dictonaryCombined=constant_new_comments_mapping,\n",
    "            json_schema=json_schema_SC,\n",
    "            model_name=\"gpt-4o\",\n",
    "            max_tokens=2000, # increase limit\n",
    "        )\n",
    "        \n",
    "        # append raw results\n",
    "        categories.append(category)\n",
    "        rawResults.append(result)\n",
    "        \n",
    "        # append parsed results\n",
    "        #parsed_section = result.get('parsed', {})\n",
    "        #mainFindings.append(parsed_section.get('mainFindings'))\n",
    "        #differences.append(parsed_section.get('differences'))\n",
    "        #summary.append(parsed_section.get('summary'))\n",
    "        #print(\"length of mainFindings:\", len(parsed_section.get('mainFindings')))\n",
    "        #print(\"length of differences:\", len(parsed_section.get('differences')))\n",
    "        #print(\"length of summary:\", len(parsed_section.get('summary')))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_section = result.get('parsed', {})\n",
    "print(parsed_section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save file\n",
    "df_RR = pd.DataFrame({\n",
    "    'Category': categories,\n",
    "    'mainFindings': mainFindings,\n",
    "    'differences': differences,\n",
    "    'summary': summary,\n",
    "    'rawResults' : rawResults\n",
    "})\n",
    "\n",
    "# Path to your Excel file\n",
    "file_path = directory + \"/output/\" + \"rescue robot_ChatGPT\" + \".xlsx\"\n",
    "# save the dataframe to an Excel file\n",
    "df_RR.to_excel(file_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for socially assistive robots (main findings, difference, summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "mainFindings = []\n",
    "differences = []\n",
    "summary = []\n",
    "rawResults = []\n",
    "\n",
    "\n",
    "for category in abbreviations_dict.keys():\n",
    "    print(f\"category: {category}\")\n",
    "    \n",
    "    # do not process the rest categories\n",
    "    if category not in ['RCPP', 'RCPN', 'RCA', 'RCN']:\n",
    "        df = all_sheets_SAR[category]\n",
    "        constant_comments_mapping = create_multivalue_dict(df, 'constant', 'constant_comments')\n",
    "        # print(\"mapping constant x comments:\", constant_comments_mapping)\n",
    "        # print(len(constant_comments_mapping))\n",
    "        \n",
    "        new_comments_mapping = create_multivalue_dict(df, 'new', 'new_comments')\n",
    "        # print(\"mapping new x comments:\", new_comments_mapping)\n",
    "        # print(len(new_comments_mapping))\n",
    "    \n",
    "        result = basic_API_call(prompt=prompt_template,\n",
    "            robots=\"socially assistive robots\",\n",
    "            topicCategory=abbreviations_dict[category],\n",
    "            openai_api_key=key.openai_api_key,\n",
    "            dictonaryRigid=constant_comments_mapping,\n",
    "            dictonarySoft=new_comments_mapping,\n",
    "            json_schema=json_schema,\n",
    "            model_name=\"gpt-4o\",\n",
    "            max_tokens=1000,\n",
    "        )\n",
    "        \n",
    "        # append raw results\n",
    "        categories.append(category)\n",
    "        rawResults.append(result)\n",
    "        \n",
    "        # append parsed results\n",
    "        parsed_section = result.get('parsed', {})\n",
    "        mainFindings.append(parsed_section.get('mainFindings'))\n",
    "        differences.append(parsed_section.get('differences'))\n",
    "        summary.append(parsed_section.get('summary'))\n",
    "        \n",
    "# save file\n",
    "df_SAR = pd.DataFrame({\n",
    "    'Category': categories,\n",
    "    'mainFindings': mainFindings,\n",
    "    'differences': differences,\n",
    "    'summary': summary,\n",
    "    'rawResults' : rawResults\n",
    "})\n",
    "\n",
    "# Path to your Excel file\n",
    "file_path = directory + \"/output/\" + \"socially assistive robot_ChatGPT\" + \".xlsx\"\n",
    "# save the dataframe to an Excel file\n",
    "df_SAR.to_excel(file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
