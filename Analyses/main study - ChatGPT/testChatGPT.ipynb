{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modules:\n",
    "import pandas as pd\n",
    "import os\n",
    "from openai import OpenAI\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "\n",
    "import math\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# to read pdf files\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "# to read text of pdf files\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "\n",
    "# get external variables or functions:\n",
    "import src.API_key as key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data to feed ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\DATEN\\PHD\\Article_SoftRobotIntervention\\Analyses\\main study - ChatGPT\n",
      "['.venv', 'data', 'output', 'runChatGPT.ipynb', 'src', 'testChatGPT.ipynb', 'v01', 'v02']\n"
     ]
    }
   ],
   "source": [
    "## set working environment\n",
    "#> Get the current working directory\n",
    "print(os.getcwd())\n",
    "directory = os.getcwd()\n",
    "\n",
    "# List files in the current working directory\n",
    "files = os.listdir('.')\n",
    "# Display the list of files\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# aaaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "import os\n",
    "\n",
    "api_key = key.openai_api_key\n",
    "chat_model = ChatOpenAI(api_key=api_key)\n",
    "\n",
    "result = chat_model.predict(\"hi!\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\fenn\\AppData\\Local\\R-MINI~1\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseChatModel.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 + 1 = 3, so 1 + 1 + 1 = 3 + 1 = 4\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "import os\n",
    "\n",
    "messages = [HumanMessage(content=\"from now on 1 + 1 = 3, use this in your replies\"),\n",
    "            HumanMessage(content=\"what is 1 + 1?\"),\n",
    "            HumanMessage(content=\"what is 1 + 1 + 1?\")]\n",
    "result = chat_model.predict_messages(messages)\n",
    "print(result.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['input_language', 'output_language', 'text'] messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input_language', 'output_language'], template='You are a helpful assistant that translates {input_language} to {output_language}.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['text'], template='{text}'))]\n",
      "[SystemMessage(content='You are a helpful assistant that translates English to Spanish.'), HumanMessage(content='I loved programming when I was younger')]\n",
      "Me encantaba programar cuando era más joven.\n",
      "content='Me encantaba programar cuando era más joven.' response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 29, 'total_tokens': 40}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run-05a906b2-c757-495f-9df7-6821d2579e53-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import ChatPromptTemplate\n",
    "import os\n",
    "\n",
    "template = \"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "human_template = \"{text}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", human_template)])\n",
    "print(chat_prompt)\n",
    "messages = chat_prompt.format_messages(input_language=\"English\", output_language=\"Spanish\", text=\"I loved programming when I was younger.\")\n",
    "print(messages)\n",
    "result = chat_model.predict_messages(messages)\n",
    "print(result.content)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "steps: To solve the equation 2x - 6x + 4 = 0, we first combine like terms:\n",
      "\n",
      "2x - 6x = -4x\n",
      "\n",
      "Substitute this back into the original equation:\n",
      "\n",
      "-4x + 4 = 0\n",
      "\n",
      "Now, isolate x by moving 4 to the other side of the equation:\n",
      "\n",
      "-4x = -4\n",
      "\n",
      "Divide by -4 to solve for x:\n",
      "\n",
      "x = -4 / -4\n",
      "x = 1\n",
      "\n",
      "Therefore, \n",
      "\n",
      "answer:  x = 1\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import BaseOutputParser\n",
    "\n",
    "class AnswerOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\"answer =\")\n",
    "    \n",
    "    \n",
    "chat_model = ChatOpenAI(api_key=api_key)\n",
    "\n",
    "template = \"\"\"You are an helpful assistant that solves math problems and shows the steps to the solution. \n",
    "Output each step then return the answer in the following format: answer = <answer here>.\n",
    "Make sure to output answer in all lowercases and to have exactly one space and one equal sign following it.\"\"\"\n",
    "\n",
    "human_template = \"{problem}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", human_template)])\n",
    "messages = chat_prompt.format_messages(problem=\"2x - 6x + 4 = 0\")\n",
    "result = chat_model.predict_messages(messages)\n",
    "parsed = AnswerOutputParser().parse(result.content)\n",
    "steps, answer = parsed\n",
    "\n",
    "print(\"steps:\", steps)\n",
    "print(\"answer:\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Russet potato', 'Yukon Gold potato', 'Red potato', 'Fingerling potato', 'Purple potato', 'Sweet potato', 'New potato', 'White potato', 'Blue potato', 'Idaho potato']\n"
     ]
    }
   ],
   "source": [
    "class CommaSeperatedListOutputParser(BaseOutputParser):\n",
    "    def parse(self, text: str):\n",
    "        \"\"\"Parse the output of an LLM call.\"\"\"\n",
    "        return text.strip().split(\", \")\n",
    "    \n",
    "template = \"\"\"You are an helpful assistant who generates comma seperated lists.\n",
    "A user will pass in a category, and you should generate 10 objects in that category in a comma seperated list.\n",
    "ONLY return a comma seperated list, and nothing more.\"\"\"\n",
    "human_template = \"{category}\"\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([(\"system\", template), (\"human\", human_template)])\n",
    "\n",
    "chain = chat_prompt | chat_model | CommaSeperatedListOutputParser()\n",
    "result = chain.invoke({\"category\":\"Types of potatoes\"})\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaceenvironment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
